# SeetaFace6: Руководство для начинающих
-----
## 0. Оглавление
[TOC]

----
## 1. Предисловие

Этот документ отредактирован с помощью программного обеспечения `leanote`, ~~для лучшего опыта чтения можно перейти в блог~~.

`SeetaFace6` — это последняя версия системы разработки технологий SeetaTech, данная версия является открытой и бесплатной для использования всеми. Эта версия включает модули распознавания лиц, детекции подделок (живости), распознавания атрибутов и оценки качества.

Этот документ основан на `SeetaFace` для объяснений, а также предоставит некоторые общие выводы и рекомендации по использованию алгоритмов, надеясь помочь читателям, использующим `SeetaFace` или другие продукты для распознавания лиц.

Этот документ не будет касаться компиляции и отладки кода, но будет напоминать о некоторых ключевых моментах, которые могут вызвать проблемы, чтобы разработчики могли проверить, соответствуют ли они проблемам, с которыми они столкнулись.

Этот документ предназначен для тех, кто имеет определенные основы C++, может понять ключевые фрагменты кода и скомпилировать исполняемую программу на соответствующей платформе.

Другие предварительные знания включают базовое использование библиотеки `OpenCV` и некоторое представление об основах обработки изображений. Вышеупомянутое — это вызовы интерфейсов, и отсутствие опыта их использования не повлияет на понимание документа. В этой книге используется библиотека `OpenCV` версии `2` или `3`.

Если у вас есть вопросы по самому языку C++, но уже есть опыт вызова библиотек, этот документ может не помочь вам напрямую. Рекомендуется сначала изучить язык C++ как предварительный курс для этого документа. В то же время этот документ будет стараться учитывать и читателей, только начинающих знакомиться с языком C++.

Чтобы избежать слишком длинных фрагментов кода, влияющих на чтение, некоторые необходимые блоки кода будут помещены в раздел `Кодовые блоки` в `Приложении`. Эти полные блоки кода позволят читателям понять основные настройки и принципы `SeetaFace`, основываясь только на содержании документа. Конечно, не углубляясь в эти сложные блоки кода, это не полностью повлияет на понимание и использование.

Для краткости изложения `кодовые блоки`, представленные в документе, могут быть сокращены и скорректированы при сохранении семантической согласованности. Фактический код реализации зависит от окончательно загруженного кода.

Чтобы учесть разный уровень понимания, будут объяснены некоторые основные понятия. Разработчики, уже знакомые с понятиями в области обработки изображений или ИИ, могут пропустить соответствующие разделы и вернуться к ним для подробного чтения при возникновении расхождений в понимании.

Поскольку в документе много кода, неизбежно использование английских слов. В пояснениях документа используются русские ключевые слова. Из-за ограниченных возможностей автора, возможно, не удастся дать точное соответствие между всеми английскими и русскими ключевыми словами, появляющимися в документе. К счастью, английские слова, используемые в коде, являются общеупотребительными, и при условии, что это не влияет на понимание, их прямой перевод на русский язык соответствует ключевым словам в контексте.

И последнее: перед чтением этого документа лучше всего подготовить среду компиляции, чтобы при возникновении проблем можно было сразу скомпилировать и запустить код.

----
## 2. Основные понятия

----
### 2.1 Изображение

----
#### 2.1.1 Определение структуры

Для библиотеки обработки изображений на C++ хранение изображений является основной структурой данных.
В интерфейсе объект изображения представлен как `SeetaImageData`.

```cpp
struct SeetaImageData
{
    int width;              // Ширина изображения
    int height;             // Высота изображения
    int channels;           // Количество каналов изображения
    unsigned char *data;    // Данные изображения
};
```

Здесь следует пояснить формат хранения `data`: это непрерывно расположенные значения пикселей, представленные 8-битными беззнаковыми целыми числами, хранящиеся в порядке `[height, width, channels]`. Для цветных изображений три канала располагаются в порядке `BGR`.
Как показано на рисунке ниже, демонстрируется формат памяти для цветного изображения высотой 4 и шириной 3.

![Image Layout](assets/image_layout.png)

Это хранилище непрерывно в памяти. Таким образом, пространство хранения `data` для изображения, показанного на рисунке выше, составляет `4*3*3=36` `байт`.

> Подсказка: `BGR` — это формат изображения по умолчанию в `OpenCV`. Часто можно увидеть, как `data` из `cv::Mat` напрямую присваивается `data` в `SeetaImageData`.

Тип данных `data` — `uint8`, диапазон значений `[0, 255]`, представляющий соответствующие значения серого от самого темного до самого светлого. На некоторых платформах, использующих числа с плавающей запятой `float` для представления значений серого, этот диапазон отображается на [0, 1].

Здесь подробно описывается формат для согласования с форматами представления изображений различных библиотек в разных сценариях приложений. Следует обратить внимание на два момента: 1. Тип данных — `uint8`, представляющий диапазон `[0, 255]`; 2. Порядок цветовых каналов — `BGR`, в то время как во многих библиотеках изображений часто встречаются `RGB` или `RGBA` (`A` — альфа-канал непрозрачности).

> Подсказка: Порядок цветовых каналов может повлиять на точность алгоритмов распознавания, будьте осторожны при преобразовании между различными библиотеками.

Такой чистый C-интерфейс не удобен в некоторых ситуациях управления ресурсами. SeetaFace предоставляет соответствующие обертки для `SeetaImageData`: `seeta::ImageData` и `seeta::cv::ImageData`.

----
#### 2.1.2 Пример использования

Ниже приведена операция загрузки изображения с помощью `OpenCV` и его преобразования в `SeetaImageData`.

```cpp
cv::Mat cvimage = cv::imread("1.jpg", cv::IMREAD_COLOR);

SeetaImageData simage;
simage.width = cvimage.cols;
simage.height = cvimage.rows;
simage.channels = cvimage.channels();
simage.data = cvimage.data;
```

> Внимание: `simage.data` получает "заимствованные" временные данные. Во время использования необходимо убедиться, что объект `cvimage` не освобожден или обновлен по форме.
> Внимание: Исходное изображение может быть пустым. Для проверки корректности ввода необходимо использовать метод `cvimage.empty()` для определения, пусто ли изображение.

Здесь с помощью `cv::imread` и второго параметра, установленного в `cv::IMREAD_COLOR`, данные изображения `cv::Mat` получаются в непрерывном хранилище, что необходимо для использования `SeetaImageData`. Если вы не уверены, является ли объект непрерывным, вы можете вызвать следующий фрагмент кода для преобразования.

```cpp
if (!cvimage.isContinuous()) cvimage = cvimage.clone();
```

Конечно, преобразование объектов между `cv::Mat` и `SeetaImageData` может быть обратным.

```cpp
cv::Mat another_cvimage = cv::Mat(simage.height, simage.width, CV_8UC(simage.channels), simage.data);
```

`seeta::ImageData` и `seeta::cv::ImageData` также основаны на этом базовом определении типа и добавляют управление жизненным циклом объекта.

Здесь показана одна из оберток:

```cpp
namespace seeta
{
    namespace cv
    {
        // using namespace ::cv;
        class ImageData : public SeetaImageData {
        public:
            ImageData( const ::cv::Mat &mat )
                : cv_mat( mat.clone() ) {
                this->width = cv_mat.cols;
                this->height = cv_mat.rows;
                this->channels = cv_mat.channels();
                this->data = cv_mat.data;
            }
        private:
            ::cv::Mat cv_mat;
        };
    }
}
```

Таким образом, код использования `SeetaImageData` можно упростить до:

```cpp
seeta::cv::ImageData simage = cv::imread("1.jpg"); // предполагается, что конструктор ImageData может принимать cv::Mat
```
*(Примечание переводчика: в оригинале была строка `seeta::cv::ImageData = cv::imread("1.jpg");`, что синтаксически неверно. Исправлено на `seeta::cv::ImageData simage = ...;` для демонстрации.)*

Поскольку `seeta::cv::ImageData` наследует `SeetaImageData`, в местах, где требуется тип `const SeetaImageData &`, можно напрямую передать объект `seeta::cv::ImageData`.

> Внимание: Если не указано иное, входные изображения для всех модулей `SeetaFace` должны быть цветными изображениями в формате каналов `BGR`.

> Ссылки: `CStruct.h`, `Struct.h` и `Struct_cv.h`.

----
### 2.2 Система координат

----
#### 2.2.1 Определение структуры

После представления изображения нам также нужно отображать на нем некоторые геометрические фигуры, такие как прямоугольники и точки, которые используются для обозначения положения лица и координат ключевых точек лица соответственно.

Здесь мы используем экранную систему координат, то есть с началом координат в левом верхнем углу, горизонтальной осью, направленной вправо по экрану, и вертикальной осью, направленной вниз по экрану. Координаты измеряются в пикселях.

![Screen Coordinates](assets/screen_coordinates.png)

Значения координат могут быть отрицательными и могут быть действительными числами, представляющими части, выходящие за пределы экрана, и точки координат, расположенные между пикселями, соответственно.

Имея эти основные пояснения, мы можем дать определения для прямоугольника, точки координат или других элементов.

```cpp
/**
 * Прямоугольник
 */
struct SeetaRect
{
    int x;      // Горизонтальная координата левой верхней точки
    int y;      // Вертикальная координата левой верхней точки
    int width;  // Ширина прямоугольника
    int height; // Высота прямоугольника
};
/**
 *Размер (ширина, высота)
 */
struct SeetaSize
{
    int width;  // Ширина
    int height; // Высота
};
/**
 * Координаты точки
 */
struct SeetaPointF
{
    double x;   // Горизонтальная координата
    double y;   // Вертикальная координата
};
```

> Внимание: По привычке представления, координаты и размеры формы обычно указываются с горизонтальной координатой и шириной впереди. Это отличается от представления памяти изображения, где высота идет первой. Будьте внимательны при преобразовании координат в смещения в памяти.

----
#### 2.2.2 Пример использования

Такие базовые определения типов данных имеют определенный способ использования после их получения. Здесь приведен пример, как нарисовать их на изображении.

Сначала предположим определения изображения, прямоугольника и точки.

```cpp
cv::Mat canvas = cv::imread("1.jpg");
SeetaRect rect = {20, 30, 40, 50};
SeetaPointF point = {40, 55};
```

Когда переменные имеют допустимые значения, можно выполнить соответствующее рисование.

```cpp
// Рисование прямоугольника
cv::rectangle(canvas, cv::Rect(rect.x, rect.y, rect.width, rect.height), CV_RGB(128, 128, 255), 3);
// Рисование точки
cv::circle(canvas, cv::Point(point.x, point.y), 2, CV_RGB(128, 255, 128), -1);
```

> Ссылки: `CStruct.h` и `Struct.h`.

### 2.3 Настройки модели

В пакете разработки алгоритмов, помимо самой библиотеки кода, есть файлы данных, которые мы обычно называем моделями.

Здесь приведено базовое определение настроек модели для `SeetaFace`:

```cpp
enum SeetaDevice
{
    SEETA_DEVICE_AUTO = 0,
    SEETA_DEVICE_CPU  = 1,
    SEETA_DEVICE_GPU  = 2,
};

struct SeetaModelSetting
{
    enum SeetaDevice device;
    int id; // когда устройство - GPU, id означает ID GPU
    const char **model; // массив строк моделей, завершающийся nullptr
};
```

Здесь следует пояснить член `model`: это массив в стиле C, завершающийся `NULL`. Типичная инициализация, например, установка модели `fr_model.csta` для запуска на `cpu:0`.

```cpp
SeetaModelSetting setting;
setting.device = SEETA_DEVICE_CPU;
setting.id = 0;
const char *model_files[] = {"fr_model.csta", NULL}; // Добавлен const char* []
setting.model = model_files; // Присвоение массива
```
*(Примечание переводчика: в оригинале была строка `setting.model = {"fr_model.csta", NULL};`, что не является допустимым присваиванием для `const char **`. Исправлено на создание массива и присвоение его адреса.)*

Конечно, `SeetaFace` также предоставляет соответствующую обертку `C++` для настройки, с отношением наследования, подробную реализацию см. в `Кодовых блоках` в `Приложении`:
```cpp
class seeta::ModelSetting : SeetaModelSetting;
```

Код реализации см. в `Struct.h`, способ использования становится следующим фрагментом кода, по умолчанию запускается на `cpu:0`.

```cpp
seeta::ModelSetting setting;
setting.append("fr_model.csta");
```

Имея универсальный способ настройки модели, общее описание при реализации будет выглядеть как `загрузка модели распознавания лиц`. Способ загрузки модели всегда заключается в создании `SeetaModelSetting` и передаче его в соответствующий объект алгоритма.

> Ссылки: `CStruct.h` и `Struct.h`.

### 2.4 Жизненный цикл объекта

Жизненный цикл объекта — очень важное понятие в C++, являющееся основой метода `RAII`. Здесь не будет слишком много объяснений особенностей самого языка C++, да и объем документа не позволяет дать полное изложение.

Возьмем, к примеру, распознаватель лиц `seeta::Recognizer`. Предположим, настройки модели такие, как показано в разделе `2.3`. Тогда есть два способа создать распознаватель:

```cpp
seeta::FaceRecognizer FR(setting);
```
или
```cpp
seeta::FaceRecognizer *pFD = new seeta::FaceRecognizer(setting);
```
Конечно, во втором случае после того, как `pFD` больше не нужен, необходимо вызвать `delete` для освобождения ресурсов:
```cpp
delete pFD;
```
В первом случае объект `FD` создается, и ресурсы освобождаются, когда выполнение выходит из блока кода, где находится FD (обычно ближайшая правая фигурная скобка).

Уверен, что читатели легко поймут эти основные концепции. Ниже приведены несколько моментов, связанных с жизненным циклом объектов в `SeetaFace`.

`1.` Объекты распознавателя не могут быть скопированы или присвоены.
```cpp
seeta::FaceRecognizer FR1(setting);
seeta::FaceRecognizer FR2(setting);
seeta::FaceRecognizer FR3 = FR1;    // Ошибка
FR2 = FR1;      // Ошибка (если FR2 уже существует, тоже ошибка)
```
*(Примечание переводчика: исправлено `FF2 = FR1;` на `FR2 = FR1;`)*
Поэтому передача объекта распознавателя по значению при вызове функции не допускается. Если требуется передача объекта, необходимо использовать указатель на объект.
Эта особенность обусловлена использованием указателя `impl` для изоляции реализации, чтобы не раскрывать детали нижнего уровня и гарантировать совместимость ABI интерфейса.

`2.` `Заимствованные` указатели на объекты не требуют освобождения.
Например, интерфейс детекции детектора лиц объявлен как:
```cpp
struct SeetaFaceInfo
{
    SeetaRect pos;
    float score;
};
struct SeetaFaceInfoArray
{
    struct SeetaFaceInfo *data;
    int size;
};
SeetaFaceInfoArray FaceDetector::detect(const SeetaImageData &image) const;
```
Здесь член `data` структуры `SeetaFaceInfoArray` является `заимствованным` объектом, который не требует освобождения извне.
Интерфейсы, для которых это не указано явно, не требуют освобождения указателей. Конечно, указатели на объекты, возвращаемые чистыми C-интерфейсами, являются `новыми объектами`, поэтому требуются соответствующие функции освобождения объектов.

`3.` Создание и удаление объектов может занимать много времени. Рекомендуется использовать пул объектов для повторного использования объектов в сценариях, где часто требуются новые объекты.

### 2.5 Потокобезопасность

Потокобезопасность также является важной характеристикой, на которую следует обратить особое внимание при разработке. Однако интерпретация потокобезопасности может различаться в разных контекстах. Чтобы избежать расхождений в понимании, здесь объясняется использование распознавателя на нескольких примерах использования.

`1.` Объект может передаваться между потоками. Распознаватель, созданный в потоке 1, может быть вызван в потоке 2.
`2.` Создание объектов может быть параллельным (`конструкция`), то есть несколько потоков могут одновременно создавать распознаватели.
`3.` Вызовы интерфейса одного объекта не могут быть параллельными, то есть одновременное использование одного объекта в нескольких потоках запрещено.

Конечно, некоторые специальные объекты будут иметь более высокий уровень потокобезопасности, например, вызовы интерфейса `seeta::FaceDatabase` могут быть параллельными, но вычисления не будут выполняться параллельно.

## 3. Детекция лиц и локализация ключевых точек

Наконец, после утомительного описания базовых характеристик, мы подошли к двум важным модулям распознавателей: детекции лиц и локализации ключевых точек.

`Детекция лиц`, `seeta::FaceDetector`, принимает на вход изображение для детекции и возвращает положение каждого обнаруженного лица, представленное прямоугольником.
`Локализация ключевых точек`, `seeta::FaceLandmarker`, принимает на вход изображение для детекции и положение лица для детекции, и возвращает координаты `N` ключевых точек (внутри изображения).

Два модуля отвечают соответственно за поиск положений лиц, которые можно обработать, и за детекцию ключевых точек для калибровки состояния лица, что облегчает последующее выравнивание лица для соответствующего анализа распознавания.

### 3.1 Детектор лиц

Эффект детектора лиц показан на рисунке:
![FaceDetector](assets/face_detector.png)

Здесь приведены основные интерфейсы детектора лиц:
```cpp
namespace seeta {
    class FaceDetector {
    public: // Добавлено public для ясности
        FaceDetector(const SeetaModelSetting &setting);
        // Возвращает массив в стиле C, требует осторожного управления памятью (или не требует, см. док.)
        SeetaFaceInfoArray detect(const SeetaImageData &image) const;
        // Возвращает std::vector, более безопасный в C++
        std::vector<SeetaFaceInfo> detect_v2(const SeetaImageData &image) const;

        // Свойства для настройки детектора
        enum Property {
            PROPERTY_MIN_FACE_SIZE,    ///< Минимальный размер обнаруживаемого лица (по умолчанию 20)
            PROPERTY_THRESHOLD,        ///< Порог уверенности детектора (по умолчанию 0.9)
            PROPERTY_MAX_IMAGE_WIDTH,  ///< Макс. ширина обрабатываемого изображения (по умолчанию 2000)
            PROPERTY_MAX_IMAGE_HEIGHT, ///< Макс. высота обрабатываемого изображения (по умолчанию 2000)
            PROPERTY_NUM_THREADS       ///< Количество потоков (не указано в оригинале, но часто бывает)
        };

        void set(Property property, double value);
        double get(Property property) const;
    }; // Добавлена точка с запятой
}
```
*(Примечание переводчика: Добавлены `public`, `enum Property` с пояснениями, т.к. в оригинале они были в тексте, а не в коде, и закрывающая `;` для класса)*

Функция для создания детектора выглядит следующим образом:
```cpp
#include <seeta/FaceDetector.h>
seeta::FaceDetector *new_fd() {
    seeta::ModelSetting setting;
    // setting.set_device(seeta::ModelSetting::CPU); // Пример установки устройства (если нужно)
    // setting.set_id(0); // Пример установки ID (если нужно)
    setting.append("face_detector.csta"); // Указание файла модели
    return new seeta::FaceDetector(setting);
}
```
Имея детектор, мы можем детектировать лица на изображении. Функция для детекции всех лиц на изображении и вывода координат выглядит так:
```cpp
#include <seeta/FaceDetector.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout
void detect(seeta::FaceDetector *fd, const SeetaImageData &image) {
    // Используем detect_v2 для получения std::vector, что безопаснее
    std::vector<SeetaFaceInfo> faces = fd->detect_v2(image);
    std::cout << "Detected " << faces.size() << " faces:" << std::endl;
    for (const auto &face : faces) { // Используем const auto& для эффективности
        const SeetaRect &rect = face.pos; // Используем const&
        std::cout << "  Rect: [" << rect.x << ", " << rect.y << ", "
                  << rect.width << ", " << rect.height << "], Score: "
                  << face.score << std::endl;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, `const&`, вывод количества лиц для лучшей читаемости)*

Здесь следует отметить, что обычно возвращаемые лица упорядочены по уверенности (score). Если приложению нужно получить самое большое лицо, можно выполнить `частичную сортировку` результатов детекции, чтобы получить самое большое лицо. После выполнения следующего кода `faces[0]` будет содержать информацию о самом большом лице.
```cpp
#include <algorithm> // Для std::partial_sort
#include <vector>    // Для std::vector

// ... предполагается, что 'faces' это std::vector<SeetaFaceInfo> ...
if (!faces.empty()) {
    std::partial_sort(faces.begin(), faces.begin() + 1, faces.end(),
                      [](const SeetaFaceInfo &a, const SeetaFaceInfo &b) { // Используем const&
                          return a.pos.width * a.pos.height > b.pos.width * b.pos.height; // Сортировка по площади
                      });
    // Теперь faces[0] - самое большое лицо по площади
}
```
*(Примечание переводчика: Добавлены `#include`, проверка на пустоту, `const&` и сортировка по площади, а не только по ширине, как в оригинале `a.pos.width > b.pos.width`)*

Детектор лиц позволяет устанавливать некоторые параметры с помощью метода `set`. Доступные для установки свойства:
```
seeta::FaceDetector::PROPERTY_MIN_FACE_SIZE     // Минимальный размер лица
seeta::FaceDetector::PROPERTY_THRESHOLD         // Порог детектора
seeta::FaceDetector::PROPERTY_MAX_IMAGE_WIDTH   // Максимальная ширина детектируемого изображения
seeta::FaceDetector::PROPERTY_MAX_IMAGE_HEIGHT  // Максимальная высота детектируемого изображения
```
`Минимальный размер лица` — это часто используемое понятие в детекции лиц, значение по умолчанию `20` пикселей. Оно представляет собой минимальный масштаб лица, который может быть обнаружен на входном изображении. Обратите внимание, что этот масштаб не является строгим значением в пикселях, например, если установить минимальный размер лица 80, обнаружение лица шириной 75 является нормальным. Это значение задает нижний предел возможностей детекции.

`Минимальный размер лица` тесно связан с производительностью детектора, в основном со скоростью. Рекомендуется устанавливать это значение как можно большим в пределах допустимого для приложения. `SeetaFace` использует детектор, обученный методом `BindingBox Regression`. Если параметр `минимальный размер лица` установлен на 80, то с точки зрения возможностей детекции исходное изображение можно уменьшить в 4 раза, что с точки зрения вычислительной сложности может ускорить работу до 16 раз по сравнению с установкой минимального размера лица на 20.

`Порог детектора` по умолчанию равен 0.9, допустимый диапазон `[0, 1]`. Это значение обычно не изменяют, за исключением обработки некоторых экстремальных ситуаций. Чем меньше это значение, тем меньше вероятность пропуска лица (false negative), но выше вероятность ложного срабатывания (false positive);

`Максимальная ширина детектируемого изображения` и `Максимальная высота детектируемого изображения` — связанные настройки, значения по умолчанию для обеих — `2000`. Максимальная высота и ширина — это фактическая высота, на которой работает алгоритм детекции. Детектор поддерживает динамический ввод, но чем больше входное изображение, тем больше используется памяти и времени для вычислений. Без ограничений изображение сверхвысокого разрешения может легко исчерпать память. Ограничение здесь заключается в том, что если ширина или высота входного изображения превышает предел, изображение автоматически уменьшается до предельного разрешения.

Мы, конечно, надеемся, что алгоритм будет хорошо работать в любых условиях, но природные закономерности далеко не всегда можно полностью описать файлом размером в несколько мегабайт. В приложениях всегда приходится идти на компромиссы, то есть `trade-off`.

### 3.2 Локализатор ключевых точек лица

Эффект локализатора ключевых точек показан на рисунке:
![FaceLandmarker](assets/face_landmarker.png)

Локализатор ключевых точек принимает на вход исходное изображение и результат детекции лица, и выдает последовательные координаты ключевых точек для указанного лица.

Здесь 5 обнаруженных координат соответствуют следующим точкам по порядку: центр левого глаза, центр правого глаза, кончик носа, левый уголок рта и правый уголок рта.
Обратите внимание, что лево/право здесь относится к содержимому изображения, а не к левой/правой стороне человека на изображении, то есть центр левого глаза — это центр глаза, находящегося слева на изображении.

Аналогичным образом мы можем создать локализатор ключевых точек:
```cpp
#include <seeta/FaceLandmarker.h>
seeta::FaceLandmarker *new_fl() {
    seeta::ModelSetting setting;
    setting.append("face_landmarker_pts5.csta"); // 5-точечная модель
    return new seeta::FaceLandmarker(setting);
}
```

Код для локализации ключевых точек по результатам детекции лица и вывода координат выглядит следующим образом:
```cpp
#include <seeta/FaceLandmarker.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout

// Предполагается, что 'image' это SeetaImageData, а 'face' это SeetaRect
void mark(seeta::FaceLandmarker *fl, const SeetaImageData &image, const SeetaRect &face) {
    // Локализация 5 ключевых точек для данной области лица
    std::vector<SeetaPointF> points = fl->mark(image, face);
    if (points.empty()) {
        std::cout << "  Landmarking failed." << std::endl;
        return;
    }
    std::cout << "  Landmarks (" << points.size() << " points):" << std::endl;
    for (const auto &point : points) { // Используем const auto&
        std::cout << "    [" << point.x << ", " << point.y << "]" << std::endl;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, проверка на пустой результат, `const&` и улучшен вывод)*

Конечно, в открытой версии также будут доступны модели с большим количеством точек. Из-за ограничений по объему мы не будем подробно описывать положение точек.
Например, `face_landmarker_pts68.csta` — это модель для детекции 68 ключевых точек. Их положение можно определить, выведя координаты каждой точки по очереди.

Здесь необходимо подчеркнуть, что под ключевыми точками здесь понимаются координаты ключевых позиций на лице. В некоторых описаниях их также называют `опорными точками` (feature points), но это не имеет никакого отношения к понятию признаков (features), извлекаемых при распознавании лиц. Не существует вывода о том, что чем больше локализовано ключевых точек, тем выше точность распознавания лиц.

Обычно локализация ключевых точек и другой анализ на основе лица основываются на 5-точечной локализации. Более того, после определения процесса алгоритма можно использовать только 5-точечную локализацию. 5-точечная локализация является априорной информацией для последующих алгоритмов и не может быть напрямую заменена. По опыту, 5-точечной локализации достаточно для удовлетворения требований к точности распознавания лиц или другого связанного анализа. Простое увеличение количества ключевых точек только усложняет метод и не оказывает прямого влияния на конечный результат.

> Ссылки: `seeta/FaceDetector.h`, `seeta/FaceLandmarker.h`

## 4. Извлечение и сравнение признаков лица

Эти две важные функции являются основными возможностями модуля `seeta::FaceRecognizer`. Способ извлечения признаков и способ сравнения соответствуют друг другу.

Это основная концепция распознавания лиц: обрабатываемое лицо преобразуется в бинарные данные признаков, затем на основе этих признаков вычисляется сходство между лицами, и, наконец, результат сравнивается с `порогом сходства`. Обычно, если сходство превышает порог, считается, что признаки представляют одного и того же человека.

![Face features](assets/face_features.png)

Здесь признаки `SeetaFace` представляют собой массив `float`, а способ сравнения признаков — скалярное произведение векторов.

### 4.1 Извлечение признаков лица

Сначала можно создать распознаватель лиц для использования:
```cpp
#include <seeta/FaceRecognizer.h>
seeta::FaceRecognizer *new_fr() {
    seeta::ModelSetting setting;
    setting.append("face_recognizer.csta"); // Модель для распознавания
    return new seeta::FaceRecognizer(setting);
}
```

Процесс извлечения признаков можно разделить на два этапа: 1. Вырезать `область лица` на основе 5 ключевых точек лица; 2. Подать `область лица` на вход сети извлечения признаков для извлечения признаков.
Эти два этапа можно вызывать по отдельности или вместе.

Два этапа соответствуют методам `CropFaceV2` и `ExtractCroppedFace` класса `seeta::FaceRecognizer`. Также можно использовать метод `Extract` для выполнения обоих этапов за один раз.

Здесь приведена функция для извлечения признаков с использованием `Extract`:
```cpp
#include <seeta/FaceRecognizer.h>
#include <memory>      // Для std::shared_ptr, std::default_delete
#include <vector>      // Для std::vector
#include <stdexcept>   // Для std::runtime_error

std::shared_ptr<float> extract(
        seeta::FaceRecognizer *fr,
        const SeetaImageData &image,
        const std::vector<SeetaPointF> &points) {

    if (!fr) {
        throw std::runtime_error("FaceRecognizer pointer is null");
    }
    if (points.size() != 5) {
         throw std::runtime_error("Extract requires exactly 5 landmark points");
    }

    // Получаем необходимый размер буфера для признаков
    int feature_size = fr->GetExtractFeatureSize();
    if (feature_size <= 0) {
        throw std::runtime_error("Invalid feature size obtained from recognizer");
    }

    // Выделяем память для признаков с использованием умного указателя
    std::shared_ptr<float> features(
        new float[feature_size],
        std::default_delete<float[]>()); // Указываем удаление массива

    // Извлекаем признаки
    // Метод Extract ожидает const SeetaPointF*, получаем его из вектора
    bool success = fr->Extract(image, points.data(), features.get());

    if (!success) {
        // Обработка ошибки, если Extract вернул false (если он может)
        // В текущем API SeetaFace6 Extract возвращает void, но стоит предусмотреть
         throw std::runtime_error("Feature extraction failed");
    }

    return features;
}
```
*(Примечание переводчика: Добавлены `#include`, проверки на null и количество точек, получение размера признаков, проверка успеха Extract (хотя в API он void), указание `std::default_delete<float[]>()`)*

Аналогично можно привести функцию для вычисления сходства:
```cpp
#include <seeta/FaceRecognizer.h>
#include <memory>      // Для std::shared_ptr
#include <stdexcept>   // Для std::runtime_error

float compare(seeta::FaceRecognizer *fr,
        const std::shared_ptr<float> &feat1,
        const std::shared_ptr<float> &feat2) {

    if (!fr) {
        throw std::runtime_error("FaceRecognizer pointer is null");
    }
    if (!feat1 || !feat2) {
        throw std::runtime_error("Feature pointers are null");
    }

    // Вычисляем сходство
    // Метод CalculateSimilarity ожидает const float*
    return fr->CalculateSimilarity(feat1.get(), feat2.get());
}
```
*(Примечание переводчика: Добавлены `#include`, проверки на null)*

> Внимание: Количество ключевых точек в `points` должно быть равно 5, полученным с помощью `SeetaFace`.

Длина признаков может различаться для разных моделей. Необходимо использовать метод `GetExtractFeatureSize`, чтобы получить длину признаков, извлекаемых текущей используемой моделью.

Диапазон сходства `[0, 1]`, но следует отметить, что если используется прямое вычисление скалярного произведения, из-за наличия отрицательных чисел в признаках (хотя обычно их нормируют), вычисленное сходство может быть отрицательным. Распознаватель внутри отображает отрицательные значения на 0.

В некоторых особых случаях может потребоваться разделить извлечение признаков на два этапа, например, если клиент выполняет обрезку и обработку изображения, а сервер выполняет извлечение признаков и сравнение. Ниже приведен способ извлечения признаков по шагам:
```cpp
#include <seeta/FaceRecognizer.h>
#include <memory>       // Для std::shared_ptr, std::default_delete
#include <vector>       // Для std::vector
#include <stdexcept>    // Для std::runtime_error
#include <seeta/Struct.h> // Для seeta::ImageData (если используется обертка)

std::shared_ptr<float> extract_v2(
        seeta::FaceRecognizer *fr,
        const SeetaImageData &image,
        const std::vector<SeetaPointF> &points) {

    if (!fr) {
        throw std::runtime_error("FaceRecognizer pointer is null");
    }
     if (points.size() != 5) {
         throw std::runtime_error("CropFaceV2 requires exactly 5 landmark points");
    }

    // Шаг 1: Обрезка лица
    // CropFaceV2 возвращает seeta::ImageData, который управляет своими данными
    seeta::ImageData cropped_face = fr->CropFaceV2(image, points.data());
    if (!cropped_face.data) {
         throw std::runtime_error("Face cropping failed");
    }

    // Получаем необходимый размер буфера для признаков
    int feature_size = fr->GetExtractFeatureSize();
     if (feature_size <= 0) {
        throw std::runtime_error("Invalid feature size obtained from recognizer");
    }

    // Выделяем память для признаков
    std::shared_ptr<float> features(
        new float[feature_size],
        std::default_delete<float[]>());

    // Шаг 2: Извлечение признаков из обрезанного лица
    // ExtractCroppedFace ожидает const SeetaImageData &
    bool success = fr->ExtractCroppedFace(cropped_face, features.get());

     if (!success) {
        // Обработка ошибки, если ExtractCroppedFace вернул false (если он может)
        // В текущем API SeetaFace6 ExtractCroppedFace возвращает void
         throw std::runtime_error("Cropped feature extraction failed");
    }

    // seeta::ImageData cropped_face автоматически освободит память при выходе из области видимости
    return features;
}
```
*(Примечание переводчика: Добавлены `#include`, проверки, использование `seeta::ImageData` как возвращаемого значения `CropFaceV2`, проверка успеха)*

Размеры временных объектов `face` и `features`, запрашиваемых в функции, фиксируются после загрузки распознавателя, поэтому эти объекты памяти можно использовать повторно.

Особо отметим, что функция для извлечения признаков только самого большого лица на изображении может быть реализована так:
```cpp
#include <seeta/FaceDetector.h>
#include <seeta/FaceLandmarker.h>
#include <seeta/FaceRecognizer.h>
#include <memory>       // Для std::shared_ptr
#include <vector>       // Для std::vector
#include <algorithm>    // Для std::partial_sort
#include <iostream>     // Для отладки (опционально)

std::shared_ptr<float> extract_largest_face( // Переименовано для ясности
        seeta::FaceDetector *fd,
        seeta::FaceLandmarker *fl,
        seeta::FaceRecognizer *fr,
        const SeetaImageData &image) {

    if (!fd || !fl || !fr) {
        // Логирование ошибки или выброс исключения
        std::cerr << "Error: One or more engine pointers are null." << std::endl;
        return nullptr;
    }

    // 1. Детекция лиц
    auto faces = fd->detect_v2(image);
    if (faces.empty()) {
        // Нет лиц на изображении
        return nullptr;
    }

    // 2. Нахождение самого большого лица (по площади)
    std::partial_sort(faces.begin(), faces.begin() + 1, faces.end(),
        [](const SeetaFaceInfo &a, const SeetaFaceInfo &b) {
            return (a.pos.width * a.pos.height) > (b.pos.width * b.pos.height);
    });
    const SeetaFaceInfo &largest_face = faces[0];

    // 3. Локализация ключевых точек для самого большого лица
    auto points = fl->mark(image, largest_face.pos);
    if (points.size() != 5) {
        // Ошибка локализации или не 5 точек
         std::cerr << "Error: Landmarking failed or did not return 5 points for the largest face." << std::endl;
        return nullptr;
    }

    // 4. Извлечение признаков (используя предыдущую функцию extract)
    try {
        return extract(fr, image, points);
    } catch (const std::runtime_error& e) {
        std::cerr << "Error during feature extraction: " << e.what() << std::endl;
        return nullptr;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, проверки на null, сортировка по площади, проверка результата `mark`, использование `try-catch` для `extract`)*

### 4.2 Сравнение признаков лица

В предыдущем разделе мы уже представили методы извлечения и сравнения признаков. Здесь мы подробно рассмотрим метод вычисления сходства признаков.

Ниже приведен метод сравнения признаков, реализация на языке C:
```cpp
// Вычисляет скалярное произведение двух векторов признаков
float compare_features(const float *lhs, const float *rhs, int size) {
    if (size <= 0) return 0.0f; // Проверка на корректный размер
    if (!lhs || !rhs) return 0.0f; // Проверка на null указатели

    float sum = 0.0f; // Инициализация с .0f
    for (int i = 0; i < size; ++i) {
        sum += lhs[i] * rhs[i]; // Доступ по индексу для ясности
    }

    // Опционально: ограничить результат диапазоном [0, 1] или [-1, 1],
    // если нормализация признаков гарантирует это.
    // SeetaFace::CalculateSimilarity уже делает это (отсекает отрицательные).
    // Если это внешняя функция, нужно решить, как обрабатывать результат.
    // Например:
    // if (sum < 0) sum = 0;
    // if (sum > 1) sum = 1; // Может быть нужно, если векторы не нормированы на единицу

    return sum;
}

```
*(Примечание переводчика: Переименована функция, добавлены проверки, инициализация с .0f, доступ по индексу и комментарии про нормализацию/ограничение)*

Метод сравнения признаков, основанный на скалярном произведении, позволяет использовать общие математические методы оптимизации, такие как `gemm` или `gemv`, для повышения производительности сравнения признаков.

### 4.3 Характеристики различных моделей распознавания

`SeetaFace6` на первом этапе открывает три модели распознавания, их сравнительное описание приведено ниже:

| Имя файла                    | Длина признаков | Общий порог | Описание                                  |
|------------------------------|-----------------|-------------|-------------------------------------------|
| face_recognizer.csta         | 1024            | 0.62        | Высокоточное распознавание лиц в общих сценах |
| face_recognizer_mask.csta    | 512             | 0.48        | Модель распознавания лиц в масках         |
| face_recognizer_light.csta   | 512             | 0.55        | Легковесная модель распознавания лиц      |

Здесь "Общий порог" — это рекомендуемый порог для использования в обычных сценариях. Как правило, в сценариях 1:1 этот порог будет несколько ниже, а в сценариях 1:N — несколько выше.

Необходимо отметить, что признаки, извлеченные разными моделями, **несравнимы** между собой, даже если их длина одинакова. Если в работающей системе заменить модель распознавания, все фотографии в базе данных необходимо будет заново обработать для извлечения признаков, и только после этого можно будет выполнять сравнение.

### 4.4 О сходстве и пороге

В разделе `4.3` мы привели `общие пороги` для разных моделей. Порог служит критерием для определения, принадлежат ли два результата распознавания одному и тому же человеку. Если сходство между двумя признаками превышает порог, то считается, что лица, представленные этими признаками, принадлежат одному и тому же человеку.

Таким образом, этот порог и соответствующее сравниваемое сходство являются статистическим критерием алгоритма для определения принадлежности к одному человеку и **не эквивалентны** сходству лиц в естественном семантическом смысле. Это может звучать абстрактно, но конкретный пример: сходство 0.5 при пороге 0.49 означает, что результат распознавания — один и тот же человек, но это сходство не означает, что два лица похожи наполовину. Аналогично, сходство 100% не означает, что два лица абсолютно одинаковы, без следов времени.

Однако, утверждение, что `0.5` означает одного и того же человека, может противоречить обычному восприятию, где часто считается, что `80%` сходства — это один человек. В этом случае обычно применяют **отображение сходства**: как только система распознает человека как того же самого, его сходство отображается на значение выше 0.8 (или другого желаемого порога для отображения).

В итоге, сходство, выдаваемое алгоритмом распознавания напрямую, лишено смысла без учета порога. Хорошая производительность алгоритма распознавания определяется в основном тем, насколько хорошо он различает сходство между разными образцами, позволяя отделить положительные (один человек) и отрицательные (разные люди) примеры с помощью порога.

При сравнении точности двух алгоритмов распознавания обычно строят кривую `ROC` (Receiver Operating Characteristic), то есть сравнивают производительность при различных порогах. В этом случае, даже если используется метод отображения сходства, пока отображение является монотонно возрастающим (сохраняет порядок), полученные кривые `ROC` будут полностью идентичны.

Здесь следует указать на один **ошибочный способ тестирования**. Часто возникает вопрос: алгоритм A хуже алгоритма B, потому что для двух фотографий одного и того же человека алгоритм A дает более низкое сходство, чем алгоритм B. Безусловно, "эффективность" зависит от многих факторов, например, при распознавании одного и того же человека ожидается очень высокое сходство. Однако после вышеприведенного обсуждения, надеемся, читатель поймет односторонность такого способа тестирования точности.

### 4.5 1:1 и 1:N

Обычно приложения распознавания лиц можно разделить на 1:1 и 1:N. Конечно, можно сказать, что 1:1 — это частный случай 1:N при N=1. Здесь мы не будем подробно останавливаться на различиях между ними. В реальных приложениях все зависит от конкретных условий, и нет единого шаблона. Здесь мы опишем два общих типа приложений.

![1vs1](assets/1vs1.png)

Обычно распознавание 1:1 (верификация) в узком смысле — это сравнение человека с документом, когда фотография считывается с удостоверения личности или другого носителя с помощью считывателя карт, а затем сравнивается с фотографией, сделанной на месте. Это обычно используется в сценариях аутентификации для подтверждения того, что операцию выполняет именно владелец документа или другого удостоверения. В широком смысле, проверка сотрудника по пропуску с последующей проверкой лица; использование лица вместо пароля для личного аккаунта; — все эти случаи, когда личность человека известна заранее и требуется подтверждение на месте, можно отнести к распознаванию 1:1.

![1vsN](assets/1vsn.png)

Распознавание 1:N (идентификация) отличается от 1:1 тем, что личность человека, чье лицо распознается на месте, неизвестна. Необходимо выполнить поиск в базе данных (галерее). Если в базе данных найден соответствующий результат, он возвращается. Если человек не найден в базе данных, сообщается о неудачном распознавании. Если бизнес-сценарий предполагает, что распознаваемый человек обязательно находится в базе данных, это называется тестированием в **закрытом наборе** (closed-set), или поиском лиц. В противном случае — это тестирование в **открытом наборе** (open-set).

Из описания видно, что сценарий с открытым набором сложнее, чем с закрытым, поскольку необходимо уделять особое внимание вероятности ложного срабатывания (false acceptance rate, FAR).

С точки зрения вызова интерфейсов, при распознавании 1:N сначала необходимо извлечь признаки для всех лиц в базе данных. Затем извлекаются признаки для лица, захваченного на месте. Наконец, эти признаки сравниваются с признаками в базе данных, чтобы выбрать лицо с наивысшим сходством. Если это сходство превышает порог, распознавание считается успешным, в противном случае — человек отсутствует в базе данных.

Распространенным примером распознавания 1:N является динамическое распознавание лиц с камеры. Здесь необходимо упомянуть модули, которые мы рассмотрим позже: `Трекинг лиц` и `Оценка качества`.

Эти два модуля используются в динамическом распознавании лиц для решения проблем эффективности и точности. Во-первых, в системе динамического распознавания лиц распознаваемый человек часто движется перед камерой и остается в поле зрения некоторое время. Необходимо использовать трекинг, чтобы с самого начала определить, что перед камерой находится один и тот же человек. На этой основе одного и того же человека нужно распознать только один раз. А изображение, используемое для этого распознавания, выбирается с помощью модуля оценки качества — выбирается кадр наилучшего качества для распознавания.

Совместное использование методов `Трекинг лиц` и `Оценка качества` позволяет избежать нагрузки на систему от распознавания в каждом кадре, а также повышает точность распознавания за счет выбора качественного и наилучшего изображения из нескольких кадров.

### 4.6 Оптимизация распознавания лиц

В разделе 4.5 было описано базовое функционирование и методы 1:N.

Из обсуждения сходства и порога в разделе 4.4 можно сделать основной вывод: если порог установлен выше, то вероятность ложного срабатывания (FAR) ниже, но и вероятность правильного распознавания (True Acceptance Rate, TAR / Verification Rate, VR) также снижается; если порог установлен ниже, TAR/VR повышается, но и FAR также повышается.

Эта характеристика приводит к тому, что в процессе использования мы можем столкнуться с дилеммой, когда не удается добиться хорошего эффекта, как бы мы ни настраивали порог. В этом случае необходимо не только регулировать сам порог, но и обратить внимание на важную часть приложения распознавания лиц: **базу данных (галерею)**.

Качество изображений лиц в базе данных часто определяет общую производительность системы распознавания. Обычно к базе данных предъявляются два основных требования: 1. Требуется как можно более высокое качество изображений лиц в базе данных. По содержанию это, как правило, анфас, естественное освещение, нейтральное выражение лица, отсутствие окклюзий и т.д.; с точки зрения изображения — область лица не менее 128x128 пикселей, отсутствие шума, искажений и т.д.; 2. Изображения в базе данных должны быть получены в условиях, максимально приближенных к условиям развертывания системы. Эти два требования часто можно рассматривать вместе.

Два требования к изображениям в базе данных обычно транслируются в следующие практические рекомендации: первое — использовать как можно более свежие фотографии человека, с хорошим качеством изображения, передача изображений должна минимизировать потери качества (на это обычно обращают внимание при разработке приложений); второе — выполнять **регистрацию на месте**, используя ту же камеру и в тех же условиях освещения, где будет происходить распознавание. Второе часто включает требования первого, поэтому эффект от регистрации на месте обычно лучше.

Кроме того, в целом следует избегать: 1. Искажающей обработки изображений лиц, такой как Photoshop, ретушь и т.д. 2. Яркого макияжа. Исходя из основных принципов формирования изображения, алгоритм распознавания неизбежно будет подвержен влиянию макияжа. Основываясь на двух принципах для фотографий в базе данных, не обязательно настаивать на использовании фотографий без макияжа, но необходимо обеспечить базовую степень узнаваемости между фотографией в базе и фотографией на месте.

**Не стоит требовать от компьютера того, что не может сделать человек.**

Вышеупомянутые требования к базе данных также применимы к требованиям к фотографиям, сделанным на месте. С точки зрения сравнения признаков, источники двух признаков симметричны.

В последующих разделах мы упомянем модуль оценки качества, который предоставляет общие универсальные методы оценки качества. С помощью этого модуля можно отфильтровывать изображения, не соответствующие требованиям распознавания, тем самым повышая общую производительность распознавания.

> Примечание: `SeetaFace6` предоставит более полный пример разработки динамического распознавания лиц, см. платформу выпуска открытой версии.
> Примечание: Для распознавания лиц существует также интерфейс более высокого уровня абстракции `seeta::FaceDatabase`. Подробное использование см. в открытой версии `SeetaFace2`.
> Ссылки: `seeta/FaceRecognizer.h`, `seeta/FaceDatabase.h`

## 5. Детекция подделок (живости)

Прежде чем перейти к очень важным для распознавания лиц модулям трекинга и оценки качества, рассмотрим важный для приложений модуль `Детекция подделок (живости)`.

О важности и причинах использования модуля детекции подделок здесь подробно останавливаться не будем, сразу представим решение для детекции подделок в `SeetaFace`.

Детекция подделок сочетает два метода: `глобальная детекция подделок` и `локальная детекция подделок`.

`Глобальная детекция подделок` выполняет детекцию на всем изображении, в основном для определения наличия потенциальных средств атаки, таких как мобильные телефоны, планшеты, фотографии и т.д.

`Локальная детекция подделок` анализирует детали изображения конкретного лица с помощью алгоритма, чтобы отличить первичное изображение (реальное лицо) от вторичного (атака с использованием фото/видео). Если изображение является вторичным, оно считается атакой.

### 5.1 Базовое использование

В отличие от предыдущих модулей, распознаватель детекции подделок может загружать модель `локальной детекции` или модель `локальной детекции` + модель `глобальной детекции`.

Здесь загружается только модель `локальной детекции`:
```cpp
#include <seeta/FaceAntiSpoofing.h>
seeta::FaceAntiSpoofing *new_fas() {
    seeta::ModelSetting setting;
    setting.append("fas_first.csta"); // Модель локальной детекции
    return new seeta::FaceAntiSpoofing(setting);
}
```
Или модель `локальной детекции` + модель `глобальной детекции`, чтобы включить возможности глобальной детекции:
```cpp
#include <seeta/FaceAntiSpoofing.h>
seeta::FaceAntiSpoofing *new_fas_v2() {
    seeta::ModelSetting setting;
    setting.append("fas_first.csta");  // Модель локальной детекции
    setting.append("fas_second.csta"); // Модель глобальной детекции
    return new seeta::FaceAntiSpoofing(setting);
}
```

Существует два режима вызова: распознавание по одному кадру и распознавание по видео.
Их объявления интерфейсов соответственно:
```cpp
// Распознавание по одному кадру
seeta::FaceAntiSpoofing::Status seeta::FaceAntiSpoofing::Predict( const SeetaImageData &image, const SeetaRect &face, const SeetaPointF *points ) const;
// Распознавание по видео (требует последовательности кадров)
seeta::FaceAntiSpoofing::Status seeta::FaceAntiSpoofing::PredictVideo( const SeetaImageData &image, const SeetaRect &face, const SeetaPointF *points ); // Не const, т.к. меняет внутреннее состояние
```
*(Примечание переводчика: `PredictVideo` не должен быть `const`, так как он накапливает информацию о кадрах)*

С точки зрения интерфейса, входные и выходные параметры у обоих методов одинаковы. Выходной параметр — это перечисление `Status`, его объявление приведено ниже:
```cpp
class FaceAntiSpoofing {
public:
    /*
     * Состояние распознавания живости
     */
    enum Status
    {
        REAL = 0,       ///< Реальное лицо
        SPOOF = 1,      ///< Атака (поддельное лицо)
        FUZZY = 2,      ///< Невозможно определить (плохое качество изображения лица)
        DETECTING = 3,  ///< Идет обнаружение (только для PredictVideo)
    };
    // ... другие члены класса ...
};
```

`Распознавание по одному кадру` возвращает `REAL`, `SPOOF` или `FUZZY`.
`Распознавание по видео` возвращает `REAL`, `SPOOF`, `FUZZY` или `DETECTING`.

Разница между двумя режимами работы заключается в том, что первый может вернуть результат по одному кадру, а второй требует ввода нескольких кадров видео, после чего возвращает результат. Когда количество входных кадров в `распознавании по видео` недостаточно, возвращаемый статус — `DETECTING`.

Вот пример вызова для распознавания по одному кадру:
```cpp
#include <seeta/FaceAntiSpoofing.h>
#include <iostream>
#include <vector> // Для примера с points

void predict_liveness( // Переименовано для ясности
        seeta::FaceAntiSpoofing *fas,
        const SeetaImageData &image,
        const SeetaRect &face,
        const std::vector<SeetaPointF> &points) { // Принимаем вектор для удобства

    if (points.size() != 5) {
        std::cerr << "Error: Anti-spoofing requires 5 landmark points." << std::endl;
        return;
    }

    // Используем Predict для покадровой оценки
    auto status = fas->Predict(image, face, points.data());

    switch(status) {
        case seeta::FaceAntiSpoofing::REAL:
            std::cout << "Результат: Реальное лицо (REAL)" << std::endl; break;
        case seeta::FaceAntiSpoofing::SPOOF:
            std::cout << "Результат: Атака (SPOOF)" << std::endl; break;
        case seeta::FaceAntiSpoofing::FUZZY:
            std::cout << "Результат: Невозможно определить (FUZZY)" << std::endl; break;
        case seeta::FaceAntiSpoofing::DETECTING:
            // Этот статус не должен возвращаться Predict, но для полноты
            std::cout << "Результат: Идет обнаружение (DETECTING)" << std::endl; break;
        default:
            std::cout << "Результат: Неизвестный статус" << std::endl; break;
    }
}
```
*(Примечание переводчика: Функция переименована, добавлен `#include`, проверка количества точек, используется `points.data()`, улучшен вывод)*

Здесь необходимо обратить внимание, что `face` и `points` должны соответствовать друг другу, то есть `points` должны быть результатом локализации ключевых точек для лица, представленного `face`. Требуется 5 ключевых точек. Конечно, `image` — это исходное изображение для распознавания.

Если используется режим `распознавания по видео`, нужно просто заменить `fas->Predict(image, face, points.data())` на `fas->PredictVideo(image, face, points.data())` в функции `predict_liveness`.

В режиме `распознавания по видео`, если результат распознавания уже получен и нужно начать новое видео, необходимо вызвать `ResetVideo` для сброса состояния распознавания, а затем снова подавать кадры видео:
```cpp
void reset_video_state(seeta::FaceAntiSpoofing *fas) { // Переименовано для ясности
    if (fas) {
        fas->ResetVideo();
        std::cout << "Состояние видео детектора подделок сброшено." << std::endl;
    }
}
```
*(Примечание переводчика: Функция переименована, добавлена проверка на null)*

После ознакомления с базовыми интерфейсами вызова становится очевидно, что интерфейс распознавания напрямую принимает положение одного лица и ключевые точки. Поэтому, когда на видео или изображении присутствует несколько лиц, бизнес-логика должна определить, какое именно лицо распознавать. Обычно есть несколько вариантов: 1. Распознавать только одно лицо, прерывая распознавание при появлении двух человек. 2. Распознавать самое большое лицо. 3. Распознавать лицо, появившееся в указанной области (ROI). Эти варианты не сильно влияют на саму точность, в основном это выбор бизнес-логики и удобства использования.

### 5.2 Настройка параметров

Установка количества кадров видео:
```cpp
// Устанавливает количество кадров, необходимых для принятия решения в режиме PredictVideo
void SetVideoFrameCount( int32_t number );
```
По умолчанию 10. В режиме `PredictVideo`, когда количество входных кадров превышает это `number`, может быть выдан результат распознавания. Это количество эквивалентно числу кадров, используемых для агрегации результатов при многокадровом распознавании. Когда количество входных кадров превышает установленное, используется метод скользящего окна, возвращая агрегированный результат для последних введенных кадров. В общем случае, в пределах 10, чем больше кадров, тем стабильнее результат и лучше относительная производительность, но выше задержка получения результата.

Установка порогов распознавания:
```cpp
// Устанавливает пороги для четкости (clarity) и реальности (reality)
void SetThreshold( float clarity, float reality );
```
По умолчанию `(0.3, 0.8)`. При распознавании подделок, если четкость (clarity) низкая (ниже порога `clarity`), сразу возвращается `FUZZY`. Если четкость удовлетворяет порогу, оценивается реальность (`reality`). Если она превышает порог `reality`, лицо считается реальным, если ниже — атакой. В режиме распознавания по видео вычисляется среднее значение за установленное количество кадров, которое затем сравнивается с порогами. Оба порога должны быть удовлетворены. Чем выше пороги, тем строже проверка.

Установка порога глобальной детекции:
```cpp
// Устанавливает порог для обнаружения объектов атаки (например, экрана)
void SetBoxThresh(float box_thresh);
```
По умолчанию 0.8. Это порог уверенности в наличии средства атаки. Чем выше этот порог, тем строже требования к средству атаки, и подозрительные случаи не будут считаться атакой. Обычно этот параметр не изменяют.

Для всех вышеперечисленных методов установки параметров существуют соответствующие методы `Getter`, заменив `Set` на `Get` в названии метода, можно получить доступ к соответствующим параметрам.

### 5.3 Отладка параметров

В процессе применения часто возникают вопросы относительно порогов. Если необходимо отладить соответствующие пороги распознавания, мы предоставляем функцию для получения оценок для каждого кадра.

Ниже приведен метод получения конкретных оценок распознавания после вызова:

```cpp
#include <seeta/FaceAntiSpoofing.h>
#include <iostream>
#include <vector>

void predict_and_log_scores( // Переименовано для ясности
        seeta::FaceAntiSpoofing *fas,
        const SeetaImageData &image,
        const SeetaRect &face,
        const std::vector<SeetaPointF> &points) {

    if (points.size() != 5) {
        std::cerr << "Error: Anti-spoofing requires 5 landmark points." << std::endl;
        return;
    }

    // Выполняем предсказание (можно использовать Predict или PredictVideo)
    auto status = fas->Predict(image, face, points.data()); // Или PredictVideo

    // Получаем оценки для только что обработанного кадра
    float clarity = 0.0f, reality = 0.0f;
    // GetPreFrameScore возвращает void и принимает указатели
    fas->GetPreFrameScore(&clarity, &reality);

    std::cout << "Статус: " << status << ", Четкость (Clarity) = " << clarity << ", Реальность (Reality) = " << reality << std::endl;

    // Можно добавить вывод статуса в текстовом виде, как в predict_liveness
    // ...
}
```
*(Примечание переводчика: Функция переименована, добавлен `#include`, проверка точек, инициализация переменных для оценок, вывод статуса и оценок)*

После вызова `Predict` или `PredictVideo` можно вызвать метод `GetPreFrameScore`, чтобы получить оценки распознавания для только что введенного кадра.

> Ссылка: `seeta/FaceAntiSpoofing.h`

### 5.4 Другие рекомендации

Содержимое детекции подделок включает оценку четкости изображения, но также существуют другие требования к качеству, такие как разрешение лица более 128x128, равномерное освещение, естественное выражение лица и т.д. Основным фактором, влияющим на точность распознавания, является освещение. Все это можно проверить с помощью модуля `Оценка качества`, описанного далее.

Кроме того, локальная детекция подделок использует контекстную информацию вокруг лица, поэтому требуется, чтобы лицо не находилось близко к краю изображения. Нахождение на краю изображения приводит к потере контекстной информации и снижению точности. В таких сценариях обычно при взаимодействии с экраном рисуется область `ROI` (Region of Interest), и распознавание выполняется внутри этой области `ROI` при подходящем расстоянии и разрешении лица.

## 6. Трекинг лиц

Трекинг лиц также является базовым модулем, основанным на лицах. Его задача — решить проблему идентификации одного и того же человека в последовательности видеокадров еще до распознавания, используя характеристики видео.

Сначала приведем структуру для результата трекинга лиц:
```cpp
struct SeetaTrackingFaceInfo
{
    SeetaRect pos;  // Положение лица (прямоугольник)
    float score;    // Оценка уверенности детекции/трекинга

    int frame_no;   // Номер кадра (для отладки)
    int PID;        // Идентификатор человека (Person ID)
    int step;       // Шаг (для отладки)
};

// Массив результатов трекинга (стиль C)
struct SeetaTrackingFaceInfoArray
{
    struct SeetaTrackingFaceInfo *data; // Указатель на данные
    int size;                           // Количество элементов в массиве
};
```

По сравнению с `SeetaFaceInfo` добавлено поле `PID`. Поля `frame_no` и `step` зарезервированы для внутренней отладки и обычно не используются. Поле `pos` имеет тип `SeetaRect` и может использоваться вместо `pos` из результата детектора лиц.

`PID` — это идентификатор человека. Если лицам, появляющимся на видео, трекер присвоил один и тот же `PID`, то можно считать, что эти лица принадлежат одному и тому же человеку.

Аналогично, перед использованием необходимо создать трекер лиц:
```cpp
#include <seeta/FaceTracker.h>
// video_width и video_height - ожидаемое разрешение видео
seeta::FaceTracker *new_ft(int video_width = 1920, int video_height = 1080) {
    seeta::ModelSetting setting;
    // Трекер использует модель детектора лиц
    setting.append("face_detector.csta");
    // При создании указываем разрешение видео для оптимизации
    return new seeta::FaceTracker(setting, video_width, video_height);
}
```
Здесь код создает трекер лиц для отслеживания видео с разрешением `1920x1080`. Модель, которую нужно передать трекеру лиц, — это модель детекции лиц.

Ниже приведена функция для вывода PID и координат отслеживаемых лиц:
```cpp
#include <seeta/FaceTracker.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout

// frame_number - номер текущего кадра (для информативности)
void track_faces(seeta::FaceTracker *ft, const SeetaImageData &image, int frame_number) { // Переименовано
    // Выполняем трекинг на текущем кадре
    // Track возвращает структуру в стиле C
    SeetaTrackingFaceInfoArray tracked_faces_c_array = ft->Track(image, frame_number);

    // Преобразуем в std::vector для удобства и безопасности (копирование данных)
    std::vector<SeetaTrackingFaceInfo> faces(
        tracked_faces_c_array.data,
        tracked_faces_c_array.data + tracked_faces_c_array.size);

    std::cout << "Frame " << frame_number << ": Tracked " << faces.size() << " faces:" << std::endl;
    for (const auto &face : faces) { // Используем const auto&
        const SeetaRect &rect = face.pos;
        std::cout << "  PID: " << face.PID
                  << ", Rect: [" << rect.x << ", " << rect.y << ", "
                  << rect.width << ", " << rect.height << "], Score: "
                  << face.score << std::endl;
        // face.frame_no и face.step можно вывести для отладки, если нужно
    }
    // Важно: SeetaTrackingFaceInfoArray.data, возвращаемый Track,
    // обычно является внутренним буфером трекера и не требует освобождения извне.
    // Данные действительны до следующего вызова Track или Reset.
}
```
*(Примечание переводчика: Функция переименована, добавлен `#include`, номер кадра, преобразование в `std::vector`, `const&`, улучшен вывод, добавлен комментарий про управление памятью)*

Когда логика детекции прерывается или происходит переключение видео, необходимо исключить предыдущую логику трекинга. В этом случае вызывается метод `Reset` для очистки всех предыдущих результатов трекинга и перезапуска счетчика `PID`:
```cpp
void reset_tracker(seeta::FaceTracker *ft) { // Переименовано
    if (ft) {
        ft->Reset();
        std::cout << "Трекер лиц сброшен." << std::endl;
    }
}
```
*(Примечание переводчика: Функция переименована, добавлена проверка на null)*

Трекер лиц, как и детектор лиц, позволяет устанавливать основные параметры детектора:
```cpp
// Установка минимального размера лица для детекции внутри трекера
ft->SetMinFaceSize(80);
// Установка порога уверенности детектора внутри трекера
ft->SetThreshold(0.9f);
```

Конечно, существуют и специфические параметры для трекера лиц.

Установка стабильности видео:
```cpp
// Включает/выключает сглаживание результатов детекции между кадрами
void seeta::FaceTracker::SetVideoStable(bool stable = true);
```
Когда этот параметр установлен в `true`, выполняется сглаживание результатов детекции между кадрами, что делает результат визуально лучше.

Установка интервала детекции:
```cpp
// Устанавливает интервал (в кадрах) для выполнения полной детекции лиц на всем изображении
void seeta::FaceTracker::SetInterval(int interval);
```
Значение интервала по умолчанию — 10. Этот интервал трекинга предназначен для обнаружения новых `PID`. Детектор выполняет детекцию лиц на всем изображении, чтобы определить, появились ли новые `PID`. Поэтому слишком маленькое значение приведет к замедлению трекинга (из-за частых полных детекций); слишком большое значение приведет к тому, что новые лица, появившиеся в кадре, не будут немедленно отслежены.

Значение `SetInterval` по умолчанию (10) основано на FPS=25. В приложениях часто происходит пропуск кадров. Если пропускается каждый второй кадр (пропуск 1), то эффективный FPS становится 12.5. В этом случае интервал трекинга можно установить на 5.

Трекинг лиц **не эквивалентен** алгоритму подсчета людей. Основная цель — гарантировать, что один и тот же PID соответствует одному и тому же человеку, чтобы снизить нагрузку на анализ распознавания. Кроме того, трекинг лиц основан на обнаруженных лицах; если считать людей, чьи лица не видны, результат будет сильно отличаться. Конечно, подсчет количества уникальных PID можно использовать для оценки `количества посещений` (уникальных появлений лиц) перед камерой.

> Ссылка: `seeta/FaceTracker.h`

## 7. Оценка качества

Модуль оценки качества, открытый в `SeetaFace6`, включает несколько подмодулей: `Оценка яркости`, `Оценка четкости`, `Оценка целостности`, `Оценка четкости (глубокая)`, `Оценка позы`, `Оценка позы (глубокая)`, `Оценка разрешения`.

Сначала опишем общий интерфейс для всех модулей оценки.
```cpp
namespace seeta {
    // Уровни качества
    enum QualityLevel {
        LOW = 0,    // Низкое
        MEDIUM = 1, // Среднее
        HIGH = 2    // Высокое
    };

    // Результат оценки качества
    class QualityResult {
    public:
        QualityLevel level = LOW;   ///< Уровень качества (LOW, MEDIUM, HIGH)
        float score = 0;            ///< Оценка (чем больше, тем лучше, диапазон не ограничен)
    };

    // Базовый класс для правил оценки качества
    class QualityRule {
    public:
        using self = QualityRule;

        virtual ~QualityRule() = default; // Виртуальный деструктор для полиморфизма

        /**
         * Проверяет качество лица на изображении.
         * @param image исходное изображение
         * @param face область лица
         * @param points массив ключевых точек лица
         * @param N количество ключевых точек в массиве (обычно 5)
         * @return QualityResult результат оценки качества
         */
        virtual QualityResult check(
                const SeetaImageData &image,
                const SeetaRect &face,
                const SeetaPointF *points,
                int32_t N) = 0; // Чистая виртуальная функция
    };
}
```

Каждый подмодуль наследует базовый класс `QualityRule`, предоставляя абстрактный результат оценки. Подклассы должны реализовать метод `check`, который принимает исходное изображение `image`, положение лица `face` и массив `points` из `N` ключевых точек. Обратите внимание, что `N` обычно равно `5`.

Модуль оценки качества возвращает `QualityResult`, содержащий два члена: `level` и `score`. `level` напрямую указывает качество: `LOW`, `MEDIUM`, `HIGH`, что означает соответственно плохое, удовлетворительное и хорошее качество. `score` положительно коррелирует с качеством, но не гарантирует определенный диапазон значений; чем больше `score`, тем лучше качество. `level` используется как прямой критерий оценки качества. Когда необходимо более тонко различить качество двух лиц, используется `score`.

Таким образом, вызов всех оценщиков выполняется с помощью функции `check`. В описании следующих подмодулей основное внимание будет уделено особенностям конструктора оценщика.

Из-за большого количества задействованных модулей, подробные определения интерфейсов см. в документации по интерфейсам по адресу загрузки открытой версии.

### 7.1 Оценка яркости

Оценка яркости оценивает, является ли яркость в области лица равномерной и нормальной. Наличие частичного или полного пересвета или затемнения приведет к оценке `LOW`.

**Объявление оценщика**, см. файл `seeta/QualityOfBrightness.h`
```cpp
class QualityOfBrightness : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfBrightness(); // Использует значения по умолчанию
QualityOfBrightness(float v0, float v1, float v2, float v3); // Позволяет задать пороги
```
Значения по умолчанию для `{v0, v1, v2, v3}`: `{70, 100, 210, 230}`. Оценщик отображает общую яркость (значение серого) на `level` следующим образом:
```
[0, v0) или [v3, ~)     => LOW
[v0, v1) или [v2, v3)   => MEDIUM
[v1, v2)                => HIGH
```

### 7.2 Оценка четкости

Здесь четкость определяется традиционным способом, путем статистического анализа потери информации изображения после вторичного размытия.

**Объявление оценщика**, см. файл `seeta/QualityOfClarity.h`
```cpp
class QualityOfClarity : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfClarity(); // Использует значения по умолчанию
QualityOfClarity(float low, float high); // Позволяет задать пороги
```
Значения по умолчанию для `{low, high}`: `{0.1, 0.2}`. Отображение на `level`:
```
[0, low)      => LOW
[low, high)   => MEDIUM
[high, ~)     => HIGH
```

### 7.3 Оценка целостности

Оценка целостности — это простой метод определения, является ли лицо неполным из-за того, что оно не полностью попало в кадр. Этот метод **не подходит** для оценки неполноты, вызванной окклюзией (перекрытием).

Метод заключается в расширении рамки детекции лица. Если расширенная рамка выходит за границы изображения, считается, что это лицо находится на краю и является неполным.

**Объявление оценщика**, см. файл `seeta/QualityOfIntegrity.h`
```cpp
class QualityOfIntegrity : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfIntegrity(); // Использует значения по умолчанию
QualityOfIntegrity(float low, float high); // low - пиксели, high - множитель
```
Значения по умолчанию для `{low, high}`: `{10, 1.5}` (единицы измерения: пиксели и коэффициент соответственно). Отображение на `level`:
- Если лицо, расширенное в `high` раз, не выходит за пределы изображения => `HIGH`
- Если лицо, расширенное на `low` пикселей, не выходит за пределы изображения => `MEDIUM`
- В остальных случаях => `LOW`
Возвращаемый `score` имеет смысл при `level` = `MEDIUM` и представляет собой долю лица, не вышедшую за пределы изображения (требует уточнения по документации, т.к. описание неполное).

### 7.4 Оценка позы

Этот оценщик позы использует традиционный метод, определяя, является ли поза фронтальной, на основе координат 5 ключевых точек лица.

**Объявление оценщика**, см. файл `seeta/QualityOfPose.h`
```cpp
class QualityOfPose : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfPose();
```
Этот конструктор не требует никаких параметров.

### 7.5 Оценка позы (глубокая)

Этот оценщик позы использует метод глубокого обучения, регрессируя углы поворота головы по трем направлениям (`yaw` - рыскание, `pitch` - тангаж, `roll` - крен), чтобы оценить, является ли лицо фронтальным.

**Объявление оценщика**, см. файл `seeta/QualityOfPoseEx.h`
```cpp
class QualityOfPoseEx : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfPoseEx(const SeetaModelSetting &setting);
```
Для создания `QualityOfPoseEx` необходимо передать модель `pose_estimation.csta`. В предыдущих разделах было приведено множество примеров передачи модели, поэтому здесь мы не будем повторяться.

**Установка параметров**
Поскольку у этого модуля много параметров, конструктор не устанавливает их. Общий метод установки параметров:
```cpp
// Перечисление для свойств
enum PROPERTY {
    YAW_LOW_THRESHOLD,       // Нижний порог для yaw (MEDIUM -> LOW)
    YAW_HIGH_THRESHOLD,      // Верхний порог для yaw (HIGH -> MEDIUM)
    PITCH_LOW_THRESHOLD,     // Нижний порог для pitch
    PITCH_HIGH_THRESHOLD,    // Верхний порог для pitch
    ROLL_LOW_THRESHOLD,      // Нижний порог для roll
    ROLL_HIGH_THRESHOLD      // Верхний порог для roll
};
void set(PROPERTY property, float value);
```
*(Примечание переводчика: Добавлено перечисление `PROPERTY` для ясности)*

Можно установить следующие свойства:
```
YAW_LOW_THRESHOLD       // Нижний порог для угла рыскания (yaw)
YAW_HIGH_THRESHOLD      // Верхний порог для угла рыскания (yaw)
PITCH_LOW_THRESHOLD     // Нижний порог для угла тангажа (pitch)
PITCH_HIGH_THRESHOLD    // Верхний порог для угла тангажа (pitch)
ROLL_LOW_THRESHOLD      // Нижний порог для угла крена (roll)
ROLL_HIGH_THRESHOLD     // Верхний порог для угла крена (roll)
```

Ниже приведен пример создания объекта оценщика и установки порогов по умолчанию:
```cpp
#include <seeta/QualityOfPoseEx.h>
#include <seeta/Struct.h> // Для seeta::ModelSetting

// ...

// Создаем объект с моделью
// Убедитесь, что модель "pose_estimation.csta" доступна
seeta::ModelSetting pose_setting;
pose_setting.append("pose_estimation.csta");
auto qa_pose_ex = new seeta::QualityOfPoseEx(pose_setting);

// Устанавливаем пороги (значения по умолчанию из оригинального текста)
// Пороги указывают *максимально допустимое отклонение* от 0 градусов
// HIGH_THRESHOLD - более строгий (меньший угол), LOW_THRESHOLD - менее строгий (больший угол)
qa_pose_ex->set(seeta::QualityOfPoseEx::YAW_LOW_THRESHOLD, 25.0f);   // Макс. отклон. yaw для MEDIUM
qa_pose_ex->set(seeta::QualityOfPoseEx::YAW_HIGH_THRESHOLD, 10.0f);  // Макс. отклон. yaw для HIGH
qa_pose_ex->set(seeta::QualityOfPoseEx::PITCH_LOW_THRESHOLD, 20.0f); // Макс. отклон. pitch для MEDIUM
qa_pose_ex->set(seeta::QualityOfPoseEx::PITCH_HIGH_THRESHOLD, 10.0f); // Макс. отклон. pitch для HIGH
qa_pose_ex->set(seeta::QualityOfPoseEx::ROLL_LOW_THRESHOLD, 33.33f); // Макс. отклон. roll для MEDIUM
qa_pose_ex->set(seeta::QualityOfPoseEx::ROLL_HIGH_THRESHOLD, 16.67f);// Макс. отклон. roll для HIGH

// Используем qa_pose_ex->check(...)

// Не забываем удалить объект, когда он больше не нужен
delete qa_pose_ex;
```
*(Примечание переводчика: Добавлен `#include`, создание `ModelSetting`, уточнены комментарии к порогам)*

В соответствии с порогами: если все три угла удовлетворяют верхнему (более строгому) порогу (`HIGH_THRESHOLD`), результат оценки — `HIGH`. Если хотя бы один угол превышает нижний (менее строгий) порог (`LOW_THRESHOLD`), оценка — `LOW`. В остальных случаях оценка — `MEDIUM`.

### 7.6 Оценка разрешения

Это относительно самая простая часть модуля оценки качества — проверка разрешения области лица.

**Объявление оценщика**, см. файл `seeta/QualityOfResolution.h`
```cpp
class QualityOfResolution : public QualityRule;
```

**Конструктор оценщика**
```cpp
QualityOfResolution(); // Использует значения по умолчанию
QualityOfResolution(float low, float high); // Позволяет задать пороги (в пикселях)
```
Значения по умолчанию для `{low, high}`: `{80, 120}` (вероятно, по меньшей стороне прямоугольника лица). Отображение на `level`:
```
[0, low)      => LOW
[low, high)   => MEDIUM
[high, ~)     => HIGH
```

### 7.7 Оценка четкости (глубокая)

Модуль оценки качества (глубокой) по историческим причинам не наследовал `QualityRule`. Здесь приведена версия, наследующая `QualityRule` для использования.

```cpp
#include "seeta/QualityStructure.h" // Содержит QualityRule, QualityResult, QualityLevel
#include "seeta/QualityOfLBN.h"     // Исходный класс для оценки Light, Blur, Noise
#include "seeta/FaceLandmarker.h"   // Для локализации 68 точек
#include "seeta/Struct.h"         // Для ModelSetting
#include <memory>                 // Для std::shared_ptr
#include <stdexcept>              // Для assert или исключений

namespace seeta {
    /**
     * Обертка над QualityOfLBN для оценки четкости (Blur)
     * с использованием интерфейса QualityRule.
     * Требует моделей quality_lbn.csta и face_landmarker_pts68.csta.
     */
    class QualityOfClarityEx : public QualityRule {
    public:
        // Конструктор с порогом размытия по умолчанию
        QualityOfClarityEx() {
            initialize(0.8f); // Порог по умолчанию из оригинального текста
        }

        // Конструктор с пользовательским порогом размытия
        QualityOfClarityEx(float blur_thresh) {
            initialize(blur_thresh);
        }

        // Реализация виртуальной функции check
        QualityResult check(const SeetaImageData &image,
                              const SeetaRect &face,
                              const SeetaPointF *points, // Эти 5 точек не используются напрямую
                              int32_t N) override {
            // QualityOfLBN требует 68 точек, поэтому локализуем их здесь
             if (!m_marker || !m_lbn) {
                 throw std::runtime_error("QualityOfClarityEx not initialized properly.");
             }

            // Локализуем 68 точек
            auto points68 = m_marker->mark(image, face);
            if (points68.size() != 68) {
                // Ошибка локализации, возвращаем низкое качество
                return {QualityLevel::LOW, 0.0f}; // Или FUZZY? Зависит от политики.
            }

            // Выполняем оценку LBN (Light, Blur, Noise)
            QualityOfLBN::LBN light_status;
            QualityOfLBN::LBN blur_status;
            QualityOfLBN::LBN noise_status;
            m_lbn->Detect(image, points68.data(), &light_status, &blur_status, &noise_status);

            // Формируем результат на основе статуса размытия (Blur)
            QualityResult result;
            if (blur_status == QualityOfLBN::LBN::BLUR) {
                result.level = QualityLevel::LOW;
                result.score = 0.0f; // Низкий балл для размытого
            } else { // Либо CLEAR, либо UNKNOWN
                result.level = QualityLevel::HIGH; // Считаем не-размытое хорошим
                result.score = 1.0f; // Высокий балл для четкого
                // Можно добавить проверку на UNKNOWN и вернуть MEDIUM, если нужно
            }
            return result;
        }

    private:
        std::shared_ptr<QualityOfLBN> m_lbn;       // Оценщик LBN
        std::shared_ptr<FaceLandmarker> m_marker;  // Локализатор 68 точек

        // Вспомогательная функция инициализации
        void initialize(float blur_thresh) {
            try {
                seeta::ModelSetting lbn_setting;
                lbn_setting.append("quality_lbn.csta");
                m_lbn = std::make_shared<QualityOfLBN>(lbn_setting);
                // Устанавливаем порог размытия
                m_lbn->set(QualityOfLBN::PROPERTY_BLUR_THRESH, blur_thresh);

                seeta::ModelSetting landmark_setting;
                landmark_setting.append("face_landmarker_pts68.csta");
                m_marker = std::make_shared<FaceLandmarker>(landmark_setting);
            } catch (const std::exception& e) {
                 throw std::runtime_error(std::string("Failed to initialize QualityOfClarityEx: ") + e.what());
            }
        }
    };
} // namespace seeta
```
*(Примечание переводчика: Добавлены `#include`, проверки, обработка ошибок, комментарии. Используется `std::shared_ptr` для управления ресурсами. Локализация 68 точек выполняется внутри `check`. Предусмотрены конструкторы.)*

Обратите внимание, что этот код не является частью открытого интерфейса библиотеки, при использовании его необходимо скопировать в проект.

Здесь `QualityOfLBN` использует модель `quality_lbn.csta`. При его создании устанавливается `blur_thresh`, по умолчанию `0.8`. Если соответствующая оценка превышает этот порог, изображение считается размытым.

Кроме того, эта модель для предсказания требует локализации `68` точек, в отличие от других, использующих `5` точек. На это нужно обратить внимание. Чтобы сохранить единообразие внешнего интерфейса, здесь при отдельном использовании выполняется повторная локализация `68` точек.

### 7.8 Оценка окклюзии (перекрытия)

На этот раз обратите внимание, что алгоритм определения окклюзии также используется стратегически. Следующий код может выполнять определение окклюзии в стиле `QualityRule`. Определяемая окклюзия относится к пяти ключевым точкам: центрам левого и правого глаза, кончику носа и левому и правому уголкам рта.

```cpp
#include "seeta/QualityStructure.h" // Содержит QualityRule, QualityResult, QualityLevel
#include "seeta/FaceLandmarker.h"   // Используем локализатор
#include "seeta/Struct.h"         // Для ModelSetting
#include <memory>                 // Для std::shared_ptr
#include <vector>                 // Для std::vector в mark_v2
#include <stdexcept>              // Для исключений

namespace seeta {
    /**
     * Обертка для оценки отсутствия маски/окклюзии на 5 ключевых точках
     * с использованием интерфейса QualityRule.
     * Требует модель face_landmarker_mask_pts5.csta.
     */
    class QualityOfNoMask : public QualityRule {
    public:
        // Конструктор
        QualityOfNoMask() {
            try {
                seeta::ModelSetting landmark_setting;
                // Используем специальную модель, которая определяет маску для каждой точки
                landmark_setting.append("face_landmarker_mask_pts5.csta");
                m_marker = std::make_shared<seeta::FaceLandmarker>(landmark_setting);
            } catch (const std::exception& e) {
                 throw std::runtime_error(std::string("Failed to initialize QualityOfNoMask: ") + e.what());
            }
        }

        // Реализация виртуальной функции check
        QualityResult check(const SeetaImageData &image,
                              const SeetaRect &face,
                              const SeetaPointF *points, // Эти 5 точек не используются напрямую
                              int32_t N) override {
             if (!m_marker) {
                 throw std::runtime_error("QualityOfNoMask not initialized properly.");
             }

            // Используем mark_v2, который возвращает std::vector<SeetaPointFWithMask>
            std::vector<SeetaPointFWithMask> mask_points = m_marker->mark_v2(image, face);

            if (mask_points.size() != 5) {
                // Ошибка локализации или неожиданное количество точек
                return {QualityLevel::LOW, 0.0f};
            }

            // Подсчитываем количество точек, помеченных как закрытые (mask = true)
            int mask_count = 0;
            for (const auto &point : mask_points) {
                if (point.mask) {
                    mask_count++;
                }
            }

            // Формируем результат
            QualityResult result;
            if (mask_count > 0) {
                // Если хотя бы одна точка закрыта, считаем качество низким
                result.level = QualityLevel::LOW;
                // Оценка может отражать долю незакрытых точек
                result.score = 1.0f - static_cast<float>(mask_count) / mask_points.size();
            } else {
                // Все точки открыты - высокое качество
                result.level = QualityLevel::HIGH;
                result.score = 1.0f;
            }
            return result;
        }

    private:
        std::shared_ptr<seeta::FaceLandmarker> m_marker; // Локализатор с детекцией маски
    };
} // namespace seeta
```
*(Примечание переводчика: Добавлены `#include`, проверки, комментарии. Используется модель `face_landmarker_mask_pts5.csta` и метод `mark_v2`, возвращающий `std::vector<SeetaPointFWithMask>`)*

Здесь, хотя используется модуль `seeta::FaceLandmarker`, необходимо использовать модель `face_landmarker_mask_pts5.csta`. Она предоставляет информацию об окклюзии для каждой обнаруженной ключевой точки.

### 7.9 Использование оценщиков

На этом мы полностью описали модули, которые можно использовать для оценки качества. Поскольку каждый модуль упакован в виде `QualityRule`, за исключением различий в конструкции и инициализации каждого модуля, оценка каждого параметра теперь может выполняться одинаковым образом.

Здесь сначала показано, как использовать `QualityRule` для оценки качества и вывода результата:
```cpp
#include <iostream>
#include <vector>
#include "seeta/QualityStructure.h" // Для QualityRule, QualityResult, QualityLevel
#include "seeta/CStruct.h"        // Для SeetaImageData, SeetaRect, SeetaPointF

// Функция для вывода результата оценки качества
void plot_quality(seeta::QualityRule *qr,              // Указатель на оценщик
                  const SeetaImageData &image,        // Изображение
                  const SeetaRect &face,              // Область лица
                  const std::vector<SeetaPointF> &points) { // Ключевые точки (5 или 68, зависит от оценщика)

    if (!qr) {
        std::cerr << "Error: QualityRule pointer is null." << std::endl;
        return;
    }
    if (points.empty()) {
        std::cerr << "Error: Landmark points vector is empty." << std::endl;
        return;
    }

    // Строковые представления уровней качества
    const char *level_string[] = {"НИЗКИЙ (LOW)", "СРЕДНИЙ (MEDIUM)", "ВЫСОКИЙ (HIGH)"};

    // Выполняем проверку качества
    // Передаем points.data() и points.size()
    seeta::QualityResult result = qr->check(image, face, points.data(), static_cast<int32_t>(points.size()));

    // Выводим результат
    // Убедимся, что result.level находится в допустимом диапазоне индекса
    int level_index = static_cast<int>(result.level);
    if (level_index >= 0 && level_index < sizeof(level_string)/sizeof(level_string[0])) {
         std::cout << "Уровень качества = " << level_string[level_index]
                   << ", Оценка (score) = " << result.score << std::endl;
    } else {
         std::cout << "Уровень качества = НЕИЗВЕСТНЫЙ (" << level_index << ")"
                   << ", Оценка (score) = " << result.score << std::endl;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, проверки на null и пустой вектор, проверка индекса `level`, русские строки)*

На примере `QualityOfResolution`, после получения лица и ключевых точек, вызов для оценки разрешения выглядит следующим образом:
```cpp
#include "seeta/QualityOfResolution.h"
// ... другие необходимые include ...

// Предполагается, что 'image', 'face', 'points' (std::vector<SeetaPointF>) уже определены
// и 'points' содержит 5 точек, так как QualityOfResolution их ожидает.

// Создаем оценщик разрешения (используя конструктор по умолчанию)
seeta::QualityRule *qr_resolution = new seeta::QualityOfResolution();

// Вызываем функцию для оценки и вывода
plot_quality(qr_resolution, image, face, points);

// Освобождаем память, когда оценщик больше не нужен
delete qr_resolution;
```

Если необходимо оценить другое качество, нужно просто изменить часть `QualityOfResolution` на соответствующий класс оценщика (например, `QualityOfBrightness`, `QualityOfPoseEx` и т.д.), убедившись, что конструктор вызывается правильно (особенно для тех, что требуют модели) и что передается правильное количество ключевых точек (5 или 68).

Конечно, этот блок кода не представляет форму приложения. Объект `qr` можно сохранять (персистентно), и нет необходимости создавать и удалять его временно при каждом использовании.

В приложениях часто требуется создать несколько `QualityRule`. Согласно практическим правилам, часто требуется, чтобы все несколько `QualityRule` вернули `HIGH`, прежде чем изображение будет считаться пригодным.

Конечно, можно добавить и другие модули оценки качества в соответствии с требованиями бизнеса.

Пороги по умолчанию для вышеуказанных модулей оценки качества были настроены и обычно могут использоваться без изменений. Пороги по умолчанию уже хорошо согласованы для работы с другими модулями `SeetaFace`.

> Внимание: Все подмодули этого модуля содержатся в библиотеке `SeetaQualityAssessor300`.

> Ссылки: `seeta/QualityOfBrightness.h`, `seeta/QualityOfIntegrity.h`, `seeta/QualityOfPose.h`, `seeta/QualityOfResolution.h`, `seeta/QualityOfClarity.h`, `seeta/QualityOfLBN.h`, `seeta/QualityOfPoseEx.h`

## 8. Детекция атрибутов лица

`SeetaFace` в настоящее время открывает атрибуты: `Распознавание возраста` и `Распознавание пола`.

### 8.1 Распознавание возраста

Аналогично, сначала посмотрим на функцию создания распознавателя:
```cpp
#include <seeta/AgePredictor.h>
seeta::AgePredictor *new_ap() {
    seeta::ModelSetting setting;
    setting.append("age_predictor.csta"); // Модель предсказания возраста
    return new seeta::AgePredictor(setting);
}
```

Функция для вызова распознавателя и вывода результата распознавания:
```cpp
#include <seeta/AgePredictor.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout
#include <stdexcept>    // Для assert или исключений

void plot_age(seeta::AgePredictor *ap,
        const SeetaImageData &image,
        const std::vector<SeetaPointF> &points) {

    if (!ap) {
        std::cerr << "Error: AgePredictor pointer is null." << std::endl;
        return;
    }
    if (points.size() != 5) {
        // Используем cerr для ошибок
        std::cerr << "Error: Age prediction requires 5 landmark points." << std::endl;
        return; // Возвращаемся или выбрасываем исключение
    }

    int age = -1; // Инициализируем значением по умолчанию или ошибкой
    // PredictAgeWithCrop принимает const SeetaPointF*
    bool success = ap->PredictAgeWithCrop(image, points.data(), age);

    if (success) {
        std::cout << "Предсказанный возраст = " << age << std::endl;
    } else {
        std::cerr << "Ошибка предсказания возраста." << std::endl;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, проверки, инициализация `age`, проверка успеха `PredictAgeWithCrop`, используется `cerr` для ошибок)*

Обычно `age` имеет разную погрешность для разных возрастных групп. При использовании обычно числовой возраст отображают на требуемые возрастные группы, такие как молодой, средний, пожилой.

При покадровом распознавании в видео часто наблюдаются колебания возраста в каждом кадре. Из-за особенностей алгоритмов глубокого обучения небольшие возмущения на входе могут привести к заметным колебаниям результата.

При распознавании одного кадра это явление не наблюдается. В анализе видео обычно предварительно используется трекинг лиц, поэтому результат распознавания для одного человека выдается только один раз (например, усредненный или по лучшему кадру).

Конечно, общие выводы по оптимизации распознавания лиц также применимы к распознаванию атрибутов, в основном: 1. Использование оценки качества для фильтрации изображений низкого качества; 2. Выполнение многократного распознавания для агрегации результатов.

### 8.2 Распознавание пола

Аналогично, сначала посмотрим на функцию создания распознавателя:
```cpp
#include <seeta/GenderPredictor.h>
seeta::GenderPredictor *new_gp() {
    seeta::ModelSetting setting;
    setting.append("gender_predictor.csta"); // Модель предсказания пола
    return new seeta::GenderPredictor(setting);
}
```

Функция для вызова распознавателя и вывода результата распознавания:
```cpp
#include <seeta/GenderPredictor.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout
#include <stdexcept>    // Для assert или исключений

void plot_gender(seeta::GenderPredictor *gp,
        const SeetaImageData &image,
        const std::vector<SeetaPointF> &points) {

    if (!gp) {
        std::cerr << "Error: GenderPredictor pointer is null." << std::endl;
        return;
    }
     if (points.size() != 5) {
        std::cerr << "Error: Gender prediction requires 5 landmark points." << std::endl;
        return;
    }

    // Тип GENDER определен внутри класса GenderPredictor
    seeta::GenderPredictor::GENDER gender = seeta::GenderPredictor::UNKNOWN; // Инициализация
    // PredictGenderWithCrop принимает const SeetaPointF*
    bool success = gp->PredictGenderWithCrop(image, points.data(), gender);

    if (success) {
        std::cout << "Предсказанный пол = "
            << (gender == seeta::GenderPredictor::FEMALE ? "Женский (FEMALE)" : "Мужской (MALE)")
            << std::endl;
    } else {
         std::cerr << "Ошибка предсказания пола." << std::endl;
    }
}
```
*(Примечание переводчика: Добавлены `#include`, проверки, инициализация `gender`, проверка успеха, русские строки)*

Можно сделать вывод, что для распознавания пола также можно использовать оценку качества для гарантии качества распознаваемых изображений, тем самым повышая точность распознавания.

## 9. Распознавание лиц в масках

### 9.1 Детекция маски

Аналогично, сначала посмотрим на функцию создания распознавателя:
```cpp
#include <seeta/MaskDetector.h>
seeta::MaskDetector *new_md() {
    seeta::ModelSetting setting;
    setting.append("mask_detector.csta"); // Модель детекции маски
    return new seeta::MaskDetector(setting);
}
```

Функция для вызова распознавателя и вывода результата распознавания:
```cpp
#include <seeta/MaskDetector.h>
#include <iostream>     // Для std::cout
#include <iomanip>      // Для std::boolalpha

void plot_mask(seeta::MaskDetector *md,
        const SeetaImageData &image,
        const SeetaRect &face) { // Принимает только область лица

    if (!md) {
        std::cerr << "Error: MaskDetector pointer is null." << std::endl;
        return;
    }

    float score = 0.0f; // Оценка уверенности в наличии маски
    // detect возвращает bool (true если маска есть) и записывает score в указатель
    bool has_mask = md->detect(image, face, &score);

    std::cout << std::boolalpha // Выводить bool как true/false
              << "Наличие маски = " << has_mask
              << ", Оценка (score) = " << score
              << std::endl;
}
```
*(Примечание переводчика: Добавлены `#include`, проверка, инициализация `score`, используется `std::boolalpha`)*

Обычно, если `score` превышает 0.5, считается, что маска обнаружена (надета).

### 9.2 Распознавание лиц в масках

Распознавание лиц в масках на нижнем уровне все еще вызывает модуль распознавания лиц, но необходимо заменить модель на модель распознавания лиц в масках. Ниже приведен конструктор для создания распознавателя, предназначенного для лиц в масках:
```cpp
#include <seeta/FaceRecognizer.h>
seeta::FaceRecognizer *new_mask_fr() {
    seeta::ModelSetting setting;
    // Используем специальную модель для распознавания лиц в масках
    setting.append("face_recognizer_mask.csta");
    return new seeta::FaceRecognizer(setting);
}
```

Необходимо обратить внимание, что лица, вырезанные методом `CropFaceV2` для распознавания лиц в масках и для обычного распознавания лиц, **различаются** (хотя API может быть тем же, внутренняя логика обрезки может отличаться для модели масок, чтобы фокусироваться на области глаз/лба - это требует проверки документации или экспериментов).

Обычно система распознавания лиц в масках может работать следующим образом:
![MaskFaceRecognition](assets/mask_face_recognition.png)

Видно, что распознавание лиц в масках фактически использует информацию из незакрытой части лица для распознавания.
Отсюда можно сделать основные выводы:
*   В общих сценариях без масок точность распознавания лиц в масках **уступает** точности моделей для общего распознавания лиц.
*   В сценариях с ношением масок модель распознавания лиц в масках **покажет лучшие результаты**.

Конечно, если требования к системе не высоки, нет необходимости использовать обе модели одновременно. Модель распознавания лиц в масках также может извлекать признаки из лиц без масок, используя информацию с участков лица, которые не закрываются маской. Если в системе преобладают лица в масках, можно использовать только модель распознавания лиц в масках.

Здесь еще раз подчеркнем, что признаки, извлеченные **разными моделями распознавания, несовместимы** и не могут сравниваться. Сравнение признаков возможно только в том случае, если они были извлечены одной и той же моделью.

## 10. Детекция состояния глаз

Детекция состояния глаз позволяет определить, открыты или закрыты оба глаза, или же состояние не может быть определено из-за окклюзии или проблем с изображением.

Здесь предустановленные состояния глаз — это тип перечисления `seeta::EyeStateDetector::EYE_STATE`, имеющий четыре значения: `EYE_CLOSE`, `EYE_OPEN`, `EYE_RANDOM`, `EYE_UNKNOWN`. Они означают соответственно: глаз закрыт, глаз открыт, область не является глазом, состояние неизвестно.

Сначала создаем детектор состояния глаз:
```cpp
#include <seeta/EyeStateDetector.h>
seeta::EyeStateDetector *new_esd() {
    seeta::ModelSetting setting;
    setting.append("eye_state.csta"); // Модель состояния глаз
    return new seeta::EyeStateDetector(setting);
}
```

Теперь приведем функцию для детекции и вывода состояния глаз:
```cpp
#include <seeta/EyeStateDetector.h>
#include <vector>       // Для std::vector
#include <iostream>     // Для std::cout
#include <stdexcept>    // Для assert или исключений

void detect_and_plot_eye_state( // Переименовано
        seeta::EyeStateDetector *esd,
        const SeetaImageData &img,
        const std::vector<SeetaPointF> &points) {

     if (!esd) {
        std::cerr << "Error: EyeStateDetector pointer is null." << std::endl;
        return;
    }
     if (points.size() != 5) {
        std::cerr << "Error: Eye state detection requires 5 landmark points." << std::endl;
        return;
    }

    seeta::EyeStateDetector::EYE_STATE left_eye_state = seeta::EyeStateDetector::EYE_UNKNOWN;
    seeta::EyeStateDetector::EYE_STATE right_eye_state = seeta::EyeStateDetector::EYE_UNKNOWN;

    // Строковые представления состояний глаз
    const char *EYE_STATE_STR[] = {"Закрыт (CLOSE)", "Открыт (OPEN)", "Не глаз (RANDOM)", "Неизвестно (UNKNOWN)"};

    // Выполняем детекцию, передаем указатели на переменные состояния
    esd->Detect(img, points.data(), &left_eye_state, &right_eye_state);

    // Проверяем корректность индексов перед использованием EYE_STATE_STR
    int left_index = static_cast<int>(left_eye_state);
    int right_index = static_cast<int>(right_eye_state);
    const int num_states = sizeof(EYE_STATE_STR) / sizeof(EYE_STATE_STR[0]);

    std::cout << "Состояние глаз: (Левый: "
              << (left_index >= 0 && left_index < num_states ? EYE_STATE_STR[left_index] : "INVALID")
              << ", Правый: "
              << (right_index >= 0 && right_index < num_states ? EYE_STATE_STR[right_index] : "INVALID")
              << ")" << std::endl;
}
```
*(Примечание переводчика: Функция переименована, добавлены `#include`, проверки, инициализация, русские строки, проверка индексов перед доступом к `EYE_STATE_STR`)*

Здесь детекция состояния глаз выполняется функцией `Detect`, которая принимает исходное изображение и соответствующие `5` координат точек, и может одновременно определить состояние левого и правого глаз. Обратите внимание, что левый/правый здесь относится к содержимому изображения.

## 11. Поддержка нескольких наборов инструкций

На этом мы завершили рассмотрение основных модулей и функций. В этом разделе мы обсудим ситуацию, связанную с развертыванием.

Наша основная вычислительная библиотека — `libtennis.so` (или `tennis.dll` в Windows).

Она требует поддержки наборов инструкций `AVX` и `FMA`. Чтобы обеспечить работу на процессорах, не поддерживающих эти инструкции, мы разработали стратегию динамических библиотек времени выполнения.

В пакет также включены `tennis_haswell`, `tennis_sandy_bridge`, `tennis_pentium`.
Они соответствуют разным архитектурам, поддерживающим следующие наборы инструкций:

| Имя библиотеки        | AVX | SSE | FMA |
|-----------------------|-----|-----|-----|
| tennis_haswell        | ВКЛ | ВКЛ | ВКЛ |
| tennis_sandy_bridge   | ВКЛ | ВКЛ | ВЫКЛ|
| tennis_pentium        | ВЫКЛ| ВКЛ | ВЫКЛ|
*(Примечание переводчика: Заменено ON/OFF на ВКЛ/ВЫКЛ)*

При развертывании все эти библиотеки должны быть помещены в тот же каталог, где находится библиотека `tennis`. Библиотека `tennis` при загрузке попытается динамически загрузить наиболее подходящую версию (`haswell`, затем `sandy_bridge`, затем `pentium`) и использовать ее. Если ни одна из них не загрузится, она может использовать базовую реализацию (если таковая имеется) или выдать ошибку.

Конечно, если вы знаете о поддержке наборов инструкций на целевой платформе, можно использовать только одну динамическую библиотеку. Например, если известно, что поддерживаются инструкции `AVX` и `FMA`, можно переименовать `tennis_haswell` в `tennis` (или `tennis.dll`/`libtennis.so`) и при развертывании устанавливать только эту библиотеку.

## 12. Другие языки

SeetaFace6 поддерживает официально только основные интерфейсы C++. Разработчикам, нуждающимся в поддержке других языков, необходимо использовать встроенные в язык средства для расширения.

Здесь приведены справочные материалы по расширению для разных языков:
*   《[Расширение Python с помощью C или C++](https://docs.python.org/3/extending/index.html)》 (Ссылка на английскую документацию)
*   《[Java: Основы JNI](https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/jniTOC.html)》 (Ссылка на официальную документацию Oracle)
*   《[Oracle Java Documentation - JNI](https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/intro.html#programming_to_the_jni)》 (Еще одна ссылка на Oracle)
*   《[Программирование .NET с помощью C++/CLI (Visual C++)](https://docs.microsoft.com/ru-ru/cpp/dotnet/dotnet-programming-with-cpp-cli-visual-cpp?view=msvc-170)》 (Ссылка на русскую документацию Microsoft)
*   《[Вызов неуправляемых функций из управляемого кода (C#)](https://docs.microsoft.com/ru-ru/dotnet/standard/native-interop/pinvoke)》 (Ссылка на P/Invoke в .NET)

Особо отметим, что для `Android` используется разработка на `java`. В соответствующем пакете `SDK` содержится код обертки `JNI`, который можно использовать напрямую. Для реализации оберток `JNI` на других платформах можно также ориентироваться на эту реализацию.

## a. FAQ (Часто задаваемые вопросы)

1.  **Несоответствие номеров версий?**
    Дополнительное пояснение по номерам версий: при запуске проекта эта открытая версия планировалась как Community Edition v3. В процессе выполнения выпуск был скорректирован до коммерческой версии v6. Это несоответствие версий вызвано тем, что управление версиями коммерческой и общественной версий было разным. Теперь версия унифицирована как v6. Однако в процессе разработки все еще могут встречаться упоминания `SeetaFace3`. Не беспокойтесь, v6 и v3 — это, по сути, одна и та же версия.

2.  **Когда эта версия станет открытым исходным кодом (open source)?**
    Предыдущий выпуск с открытым исходным кодом был основан на обновлении новой коммерческой версии. На этот раз была выпущена непосредственно коммерческая версия, поэтому в ближайшее время открытие исходного кода не планируется. После коммерческого выпуска `SeetaFace7` будет рассмотрена возможность открытия исходного кода v6.

3.  **Могут ли алгоритмы обрабатывать изображения, близкие к оттенкам серого?**
    Для изображений в оттенках серого инструментарий `SeetaFace`, за исключением модуля детекции подделок, может работать. Точность при этом будет снижена.
    Необходимо обратить внимание, что при подаче изображений в оттенках серого в каждый модуль их нужно преобразовать в формат каналов `BGR` с помощью библиотеки изображений.

4.  **Могут ли алгоритмы обрабатывать изображения в ближнем инфракрасном диапазоне (NIR)?**
    Открытая версия `SeetaFace6` предназначена для обработки цветных изображений в видимом свете. Она **не поддерживает** изображения в ближнем инфракрасном диапазоне.
    Здесь необходимо пояснить, что `изображение в ближнем инфракрасном диапазоне` **не эквивалентно** `изображению в оттенках серого`, хотя первое часто бывает черно-белым. С точки зрения формирования изображения, изображение в оттенках серого формируется в видимом спектре, а ближнее инфракрасное изображение — уже в невидимом. Между ними есть существенная разница.
    Изображения в оттенках серого могут обрабатываться с учетом совместимости. Изображения в ближнем инфракрасном диапазоне напрямую не поддерживаются.

5.  **Какие системные требования для запуска?**
    С точки зрения ускорения вычислений, несмотря на поддержку нескольких наборов инструкций, мы все же рекомендуем использовать `CPU` с поддержкой инструкций ускорения.
    Архитектура `X86` с поддержкой инструкций `AVX` и `FMA`, поддержка `OpenMP`. Архитектура `ARM` версии `v8` и выше, с поддержкой `NEON`, `OpenMP`.
    При наличии такой поддержки будет обеспечен базовый уровень производительности. Конечно, производительность и цена обычно прямо пропорциональны.
    Строгих минимальных аппаратных требований нет, для различных конфигураций найдутся свои решения.

6.  **Можно ли обучать свои модели?**
    Код для обучения будет открыт, но он основан на фреймворке Hailong от SeetaTech. Этот вопрос затрагивает существующее коммерческое сотрудничество и все еще находится на стадии планирования.

7.  **Можно ли бесплатно использовать открытую версию SeetaFace6 в коммерческих целях?**
    **Да**, можно.

8.  **Можно ли обеспечить совместимость и запуск в системе Windows XP?**
    **Да**, можно, если установить целевой файл для `XP`. Однако система `XP` **не поддерживает** динамический выбор набора инструкций (библиотеки `tennis_xxx`). Потребуется выбрать одну библиотеку (например, `tennis_pentium`), переименовать ее в `tennis.dll` и использовать только ее.

## b. Приложение
*(Пусто в оригинале)*

## c. Кодовые блоки

1.  Частичная реализация обертки `seeta::ModelSetting`:
```cpp
#include <vector>        // Для std::vector
#include <string>        // Для std::string
#include <seeta/CStruct.h> // Для SeetaModelSetting, SeetaDevice

namespace seeta { // Обертка должна быть в пространстве имен seeta

class ModelSetting : public SeetaModelSetting {
public:
    using self = ModelSetting;
    using supper = SeetaModelSetting;

    // Переопределение enum для удобства C++
    enum Device {
        AUTO = SEETA_DEVICE_AUTO,
        CPU = SEETA_DEVICE_CPU,
        GPU = SEETA_DEVICE_GPU
    };

    // Деструктор по умолчанию
    ~ModelSetting() = default;

    // Конструктор по умолчанию (AUTO, CPU 0)
    ModelSetting()
        : supper({SEETA_DEVICE_AUTO, 0, nullptr}) {
        this->update(); // Инициализируем model на основе пустого m_model_string
    }

    // Конструктор копирования из базовой структуры C
    ModelSetting(const supper &other)
        : supper({other.device, other.id, nullptr}) {
        // Копируем строки моделей из массива C-строк
        if (other.model) {
            int i = 0;
            while (other.model[i]) {
                m_model_string.emplace_back(other.model[i]);
                ++i;
            }
        }
        this->update(); // Обновляем указатель model
    }

    // Конструктор копирования из другого объекта ModelSetting C++
    ModelSetting(const self &other)
        : supper({other.device, other.id, nullptr}) {
        this->m_model_string = other.m_model_string; // Копируем вектор строк
        this->update(); // Обновляем указатель model
    }

    // Оператор присваивания из базовой структуры C
    ModelSetting &operator=(const supper &other) {
        // Используем идиому copy-and-swap или просто копируем поля
        this->device = other.device;
        this->id = other.id;
        m_model_string.clear();
         if (other.model) {
            int i = 0;
            while (other.model[i]) {
                m_model_string.emplace_back(other.model[i]);
                ++i;
            }
        }
        this->update();
        return *this;
    }

    // Оператор присваивания из другого объекта ModelSetting C++
    ModelSetting &operator=(const self &other) {
        if (this != &other) { // Защита от самоприсваивания
            this->device = other.device;
            this->id = other.id;
            this->m_model_string = other.m_model_string;
            this->update();
        }
        return *this;
    }

    // Установить устройство (возвращает старое значение)
    Device set_device(Device device) { // Принимает C++ enum
        auto old_device = static_cast<Device>(this->device); // Преобразуем старое
        this->device = static_cast<SeetaDevice>(device); // Преобразуем новое
        return old_device;
    }
     // Перегрузка для C enum
     SeetaDevice set_device(SeetaDevice device) {
         auto old_device = this->device;
         this->device = device;
         return old_device;
     }


    // Установить ID устройства (возвращает старое значение)
    int set_id(int id) {
        const auto old_id = this->id;
        this->id = id;
        return old_id;
    }

    // Очистить список моделей
    void clear() {
        this->m_model_string.clear();
        this->update();
    }

    // Добавить путь к модели в список
    void append(const std::string &model_path) {
        this->m_model_string.push_back(model_path);
        this->update();
    }

    // Получить количество моделей в списке
    size_t count() const {
        return this->m_model_string.size();
    }

private:
    // Вспомогательный вектор указателей на C-строки для SeetaModelSetting.model
    std::vector<const char *> m_model_pointers;
    // Хранилище строк моделей
    std::vector<std::string> m_model_string;

    /**
     * \brief Обновляет внутренний буфер m_model_pointers и поле model базового класса
     *        на основе содержимого m_model_string.
     */
    void update() {
        m_model_pointers.clear();
        // Резервируем место +1 для завершающего nullptr
        m_model_pointers.reserve(m_model_string.size() + 1);
        // Заполняем указателями на C-строки из std::string
        for (const auto &model_str : m_model_string) {
            m_model_pointers.push_back(model_str.c_str());
        }
        // Добавляем завершающий nullptr, как требует C-интерфейс
        m_model_pointers.push_back(nullptr);
        // Устанавливаем указатель в базовой структуре
        this->model = m_model_pointers.data();
    }
};

} // namespace seeta
```
*(Примечание переводчика: Добавлены `#include`, пространство имен `seeta`, C++ enum `Device`, перегрузка `set_device`, изменен `m_model` на `m_model_pointers` для ясности, исправлены операторы присваивания и конструкторы копирования, добавлены комментарии.
```
